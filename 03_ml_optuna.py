import os
import glob
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.metrics import (
    confusion_matrix, classification_report, f1_score, matthews_corrcoef,
    precision_score, recall_score, roc_auc_score, roc_curve, auc,
    balanced_accuracy_score, ConfusionMatrixDisplay
)
from sklearn.model_selection import train_test_split, StratifiedKFold, learning_curve, cross_val_score
from sklearn.feature_selection import RFECV, SelectKBest, f_classif
from sklearn.inspection import permutation_importance
from sklearn.preprocessing import LabelEncoder, StandardScaler

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings('ignore')

# Global Variable
RANDOM_STATE = 42
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 12


# ========================================
# Data Processing
# ========================================
def data_processing():
    """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬"""
    csv_files = glob.glob('./features_xlsx_1/*.xlsx')
    print(f"\nğŸ“‚ ë¶„ì„í•  íŒŒì¼ ìˆ˜ - {len(csv_files)}ê°œ")

    if len(csv_files) == 0:
        raise FileNotFoundError("./features_xlsx_1/ ê²½ë¡œì— xlsx íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    df_all = pd.concat([pd.read_excel(file, sheet_name=4) for file in csv_files], ignore_index=True)

    # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
    print(f"ì´ ë°ì´í„° ìˆ˜: {len(df_all)}")
    print(f"ì»¬ëŸ¼ ìˆ˜: {df_all.shape[1]}")

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    print(f"ê²°ì¸¡ì¹˜ ê°œìˆ˜: {df_all.isnull().sum().sum()}")
    if df_all.isnull().sum().sum() > 0:
        df_all = df_all.fillna(df_all.median(numeric_only=True))

    # ë¼ë²¨ ì¸ì½”ë”©
    y_all = LabelEncoder().fit_transform(df_all['label'])
    print(f"ë¼ë²¨ ë¶„í¬ - 0: {(y_all == 0).sum()}ê°œ / 1: {(y_all == 1).sum()}ê°œ")

    # ìˆ˜ì¹˜í˜• íŠ¹ì„±ë§Œ ì„ íƒ
    feature_cols = df_all.select_dtypes(include=['float64', 'int64']).columns.drop('label', errors='ignore')
    raw_features = df_all[feature_cols]

    # ë¬´í•œê°’ ì²˜ë¦¬
    raw_features = raw_features.replace([np.inf, -np.inf], np.nan)
    raw_features = raw_features.fillna(raw_features.median())

    print(f"ìµœì¢… íŠ¹ì„± ìˆ˜: {raw_features.shape[1]}ê°œ")

    X_train_raw, X_test_raw, y_train, y_test = train_test_split(
        raw_features, y_all, test_size=0.2, stratify=y_all, random_state=RANDOM_STATE
    )

    return df_all, X_train_raw, X_test_raw, y_train, y_test, feature_cols


# ========================================
# Feature Selection: Simple Correlation
# ========================================
def remove_high_correlation_features(X, threshold=0.9):
    """ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ì œê±° (ê°„ë‹¨í•œ ë°©ë²•)"""
    print(f"\nğŸ” ìƒê´€ê³„ìˆ˜ {threshold} ì´ìƒì¸ íŠ¹ì„± ì œê±° ì¤‘...")

    corr_matrix = X.corr().abs()
    upper_triangle = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )

    # ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ íŠ¹ì„± ì°¾ê¸°
    to_drop = [column for column in upper_triangle.columns
               if any(upper_triangle[column] > threshold)]

    print(f"ì œê±°í•  íŠ¹ì„±: {len(to_drop)}ê°œ")
    if len(to_drop) > 0:
        print(f"ì œê±°ëœ íŠ¹ì„±ë“¤: {to_drop[:10]}{'...' if len(to_drop) > 10 else ''}")

    return [col for col in X.columns if col not in to_drop]


# ========================================
# Feature Selection: Statistical
# ========================================
def select_best_features(X, y, k=50):
    """í†µê³„ì  ë°©ë²•ìœ¼ë¡œ ìµœê³  íŠ¹ì„± ì„ íƒ"""
    if X.shape[1] <= k:
        print(f"íŠ¹ì„± ìˆ˜({X.shape[1]})ê°€ ì´ë¯¸ ì ìœ¼ë¯€ë¡œ íŠ¹ì„± ì„ íƒ ìƒëµ")
        return X.columns.tolist()

    print(f"\nğŸ“Š {X.shape[1]}ê°œ íŠ¹ì„±ì—ì„œ ìƒìœ„ {k}ê°œ ì„ íƒ ì¤‘...")

    selector = SelectKBest(score_func=f_classif, k=k)
    selector.fit(X, y)

    selected_features = X.columns[selector.get_support()].tolist()
    print(f"âœ… {len(selected_features)}ê°œ íŠ¹ì„± ì„ íƒ ì™„ë£Œ")

    return selected_features


# ========================================
# Alternative RFECV (Optional)
# ========================================
def simple_rfecv(X, y, max_features=30):
    """ê°„ë‹¨í•œ RFECV (ì›í•œë‹¤ë©´ ì‚¬ìš©)"""
    print(f"\nğŸ” RFECV ì‹¤í–‰ ì¤‘... (ì…ë ¥ íŠ¹ì„± ìˆ˜: {X.shape[1]})")

    if X.shape[1] > max_features:
        # ë¨¼ì € í†µê³„ì  ë°©ë²•ìœ¼ë¡œ ì¤„ì´ê¸°
        selected_features = select_best_features(X, y, k=max_features)
        X = X[selected_features]
        print(f"ì‚¬ì „ ì„ íƒìœ¼ë¡œ {max_features}ê°œ íŠ¹ì„±ìœ¼ë¡œ ì œí•œ")

    # ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ RFECV ì‹¤í–‰
    estimator = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)
    rfecv = RFECV(
        estimator=estimator,
        step=3,
        cv=cv,
        scoring='f1',
        n_jobs=-1,
        min_features_to_select=5
    )

    try:
        rfecv.fit(X, y)
        selected_features = X.columns[rfecv.support_].tolist()
        print(f"ğŸ¯ RFECVë¡œ ì„ íƒëœ ë³€ìˆ˜ {len(selected_features)}ê°œ")

        # RFECV ê²°ê³¼ ì‹œê°í™”
        plt.figure(figsize=(12, 6))
        plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1),
                 rfecv.cv_results_['mean_test_score'], marker='o', linewidth=2)
        plt.axvline(x=rfecv.n_features_, color='red', linestyle='--',
                    label=f'Optimal: {rfecv.n_features_} features')
        plt.xlabel("Number of Selected Features")
        plt.ylabel("Cross-Validation F1 Score")
        plt.title("RFECV Feature Selection")
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        return selected_features

    except Exception as e:
        print(f"RFECV ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í†µê³„ì  ë°©ë²•ìœ¼ë¡œ ëŒ€ì²´
        return select_best_features(X, y, k=20)


# ========================================
# Create Default Models
# ========================================
def create_default_models():
    """ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë“  ëª¨ë¸ ìƒì„±"""

    models = {
        # ë¡œì§€ìŠ¤í‹± íšŒê·€ (ê¸°ë³¸ê°’)
        'Logistic Regression': LogisticRegression(
            random_state=RANDOM_STATE,
            max_iter=1000
        ),

        # ê²°ì • íŠ¸ë¦¬ (ê¸°ë³¸ê°’)
        'Decision Tree': DecisionTreeClassifier(
            random_state=RANDOM_STATE
        ),

        # ëœë¤ í¬ë ˆìŠ¤íŠ¸ (ê¸°ë³¸ê°’)
        'Random Forest': RandomForestClassifier(
            random_state=RANDOM_STATE,
            n_jobs=-1
        ),

        # XGBoost (ê¸°ë³¸ê°’)
        'XGBoost': XGBClassifier(
            random_state=RANDOM_STATE,
            eval_metric='logloss',
            verbosity=0,
            n_jobs=-1
        ),

        # LightGBM (ê¸°ë³¸ê°’)
        'LightGBM': LGBMClassifier(
            random_state=RANDOM_STATE,
            verbosity=-1,
            n_jobs=-1
        ),

        # SVM (ê¸°ë³¸ê°’)
        'Support Vector Machine': SVC(
            probability=True,
            random_state=RANDOM_STATE
        )
    }

    print(f"ğŸ“¦ {len(models)}ê°œ ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    return models


# ========================================
# Metrics
# ========================================
def compute_metrics(y_true, y_pred, y_proba=None):
    """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
    cm = confusion_matrix(y_true, y_pred)
    if cm.size == 4:  # 2x2 confusion matrix
        tn, fp, fn, tp = cm.ravel()
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    else:
        specificity = sensitivity = 0

    metrics = {
        'Accuracy': (y_true == y_pred).mean(),
        'Precision': precision_score(y_true, y_pred, zero_division=0),
        'Recall': recall_score(y_true, y_pred, zero_division=0),
        'F1': f1_score(y_true, y_pred, zero_division=0),
        'Balanced_Accuracy': balanced_accuracy_score(y_true, y_pred),
        'Specificity': specificity,
        'Sensitivity': sensitivity,
        'MCC': matthews_corrcoef(y_true, y_pred)
    }

    if y_proba is not None:
        try:
            metrics['AUC'] = roc_auc_score(y_true, y_proba)
        except:
            metrics['AUC'] = 0
    else:
        metrics['AUC'] = 0

    return metrics


# ========================================
# Cross Validation
# ========================================
def cross_validate_models(models, X, y, cv_folds=5):
    """êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ë“¤ ì„±ëŠ¥ ë¹„êµ"""

    print(f"\nğŸ”„ {cv_folds}-Fold êµì°¨ ê²€ì¦ ì‹¤í–‰ ì¤‘...")

    cv_results = {}
    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)

    for name, model in models.items():
        print(f"  âš¡ {name} ê²€ì¦ ì¤‘...")

        try:
            # F1 Score ê¸°ì¤€ êµì°¨ ê²€ì¦
            f1_scores = cross_val_score(model, X, y, cv=cv, scoring='f1', n_jobs=-1)

            cv_results[name] = {
                'CV_F1_mean': f1_scores.mean(),
                'CV_F1_std': f1_scores.std(),
                'CV_F1_scores': f1_scores
            }

            print(f"    CV F1: {f1_scores.mean():.4f} Â± {f1_scores.std():.4f}")

        except Exception as e:
            print(f"    âŒ {name} ê²€ì¦ ì‹¤íŒ¨: {e}")
            cv_results[name] = {
                'CV_F1_mean': 0,
                'CV_F1_std': 0,
                'CV_F1_scores': [0]
            }

    return cv_results


# ========================================
# Model Evaluation
# ========================================
def evaluate_model(model, model_name, X_train, X_test, y_train, y_test):
    """ê°œë³„ ëª¨ë¸ í‰ê°€"""

    print(f"\nğŸ“Š {model_name} í‰ê°€ ì¤‘...")

    try:
        # ëª¨ë¸ í•™ìŠµ
        model.fit(X_train, y_train)

        # ì˜ˆì¸¡
        y_pred = model.predict(X_test)

        # í™•ë¥  ì˜ˆì¸¡
        y_proba = None
        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_test)[:, 1]
        elif hasattr(model, 'decision_function'):
            y_proba = model.decision_function(X_test)

        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        metrics = compute_metrics(y_test, y_pred, y_proba)

        print(f"  âœ… F1: {metrics['F1']:.4f}, AUC: {metrics['AUC']:.4f}, Accuracy: {metrics['Accuracy']:.4f}")

        return metrics, y_pred, y_proba, model

    except Exception as e:
        print(f"  âŒ {model_name} í‰ê°€ ì‹¤íŒ¨: {e}")
        return None, None, None, None


# ========================================
# Visualization Functions
# ========================================
def plot_confusion_matrix(y_true, y_pred, model_name):
    """Confusion Matrix ì‹œê°í™”"""
    plt.figure(figsize=(8, 6))
    ConfusionMatrixDisplay.from_predictions(
        y_true, y_pred, display_labels=['Beginner', 'Trained'],
        cmap='Blues', values_format='d'
    )
    plt.title(f'Confusion Matrix - {model_name}')
    plt.tight_layout()
    plt.show()


def plot_roc_curve(y_true, y_proba, model_name):
    """ROC Curve ì‹œê°í™”"""
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, linewidth=2, label=f'AUC = {roc_auc:.3f}')
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
    plt.title(f'ROC Curve - {model_name}')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()


def plot_combined_roc_curves(results, X_test, y_test):
    """ëª¨ë“  ëª¨ë¸ì˜ ROC Curve í†µí•© ì‹œê°í™”"""
    plt.figure(figsize=(12, 8))
    colors = ['red', 'orange', 'green', 'blue', 'purple', 'brown']

    for i, result in enumerate(results):
        if result['Best Model'] is None:
            continue

        model = result['Best Model']
        name = result['Model']

        try:
            if hasattr(model, "predict_proba"):
                y_prob = model.predict_proba(X_test)[:, 1]
            elif hasattr(model, "decision_function"):
                y_prob = model.decision_function(X_test)
            else:
                continue

            fpr, tpr, _ = roc_curve(y_test, y_prob)
            roc_auc = auc(fpr, tpr)
            color = colors[i % len(colors)]
            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})',
                     color=color, linewidth=2)
        except Exception as e:
            print(f"{name} ROC ê³¡ì„  ìƒì„± ì˜¤ë¥˜: {e}")
            continue

    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=1)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves - All Models (ê¸°ë³¸ íŒŒë¼ë¯¸í„°)')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()


def plot_model_comparison(results_df):
    """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸"""
    # F1 Score ê¸°ì¤€ ì •ë ¬
    results_df_sorted = results_df.sort_values('F1', ascending=True)

    plt.figure(figsize=(12, 8))

    # ë°” ì°¨íŠ¸
    bars = plt.barh(results_df_sorted['Model'], results_df_sorted['F1'])

    # ìƒ‰ìƒ ì ìš©
    colors = plt.cm.viridis(np.linspace(0, 1, len(bars)))
    for bar, color in zip(bars, colors):
        bar.set_color(color)

    plt.xlabel('F1 Score')
    plt.title('ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (ê¸°ë³¸ íŒŒë¼ë¯¸í„°)')
    plt.grid(True, alpha=0.3)

    # ê°’ í‘œì‹œ
    for i, bar in enumerate(bars):
        width = bar.get_width()
        plt.text(width + 0.005, bar.get_y() + bar.get_height() / 2,
                 f'{width:.3f}', ha='left', va='center')

    plt.tight_layout()
    plt.show()


def plot_learning_curve(estimator, X, y, model_name):
    """Learning Curve ì‹œê°í™”"""
    try:
        train_sizes, train_scores, val_scores = learning_curve(
            estimator, X, y, cv=3, scoring='f1', n_jobs=-1,
            train_sizes=np.linspace(0.1, 1.0, 10)
        )

        train_mean = train_scores.mean(axis=1)
        train_std = train_scores.std(axis=1)
        val_mean = val_scores.mean(axis=1)
        val_std = val_scores.std(axis=1)

        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, train_mean, 'o-', label='Training Score', linewidth=2)
        plt.plot(train_sizes, val_mean, 'o-', label='Validation Score', linewidth=2)
        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)
        plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2)
        plt.title(f'Learning Curve - {model_name}')
        plt.xlabel('Training Set Size')
        plt.ylabel('F1 Score')
        plt.legend(loc='lower right')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
    except Exception as e:
        print(f"Learning curve ìƒì„± ì˜¤ë¥˜: {e}")


def plot_feature_importance(model, X, y, feature_names, model_name):
    """Feature Importance ì‹œê°í™”"""
    # ë‚´ì¬ì  ì¤‘ìš”ë„ (ìˆëŠ” ê²½ìš°)
    if hasattr(model, 'feature_importances_'):
        imp = model.feature_importances_
        sorted_idx = np.argsort(imp)[::-1]

        plt.figure(figsize=(12, 8))
        top_features = min(15, len(feature_names))
        sns.barplot(x=imp[sorted_idx][:top_features],
                    y=np.array(feature_names)[sorted_idx][:top_features])
        plt.title(f'Feature Importance - {model_name}')
        plt.xlabel('Importance Score')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

    # Permutation Importance
    try:
        result = permutation_importance(model, X, y, n_repeats=3,
                                        random_state=RANDOM_STATE, n_jobs=-1)
        idx = result.importances_mean.argsort()[::-1]

        plt.figure(figsize=(12, 8))
        top_features = min(15, len(feature_names))
        sns.barplot(x=result.importances_mean[idx][:top_features],
                    y=np.array(feature_names)[idx][:top_features])
        plt.title(f'Permutation Importance - {model_name}')
        plt.xlabel('Importance Score')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
    except Exception as e:
        print(f"Permutation importance ê³„ì‚° ì˜¤ë¥˜: {e}")


def plot_decision_tree(model, feature_names):
    """Decision Tree êµ¬ì¡° ì‹œê°í™”"""
    if hasattr(model, 'tree_'):
        plt.figure(figsize=(30, 10))
        plot_tree(model, feature_names=feature_names,
                  class_names=['Beginner', 'Trained'],
                  filled=True, rounded=True, fontsize=8, max_depth=3)
        plt.title("Decision Tree Structure (Max Depth 3)")
        plt.tight_layout()
        plt.show()
        print(f"ì‹¤ì œ Decision Tree ê¹Šì´: {model.get_depth()}")


# ========================================
# Main Pipeline
# ========================================
def main():
    """ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸"""

    print("ğŸš€ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    print("ğŸ¯ íŠ¹ì§•: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì—†ì´ ê¸°ë³¸ê°’ë§Œ ì‚¬ìš©")
    print("âš¡ ì¥ì : ë¹ ë¥´ê³  ê°„ë‹¨í•˜ë©° ëŒ€ë¶€ë¶„ ê²½ìš°ì— ì¶©ë¶„í•œ ì„±ëŠ¥")
    print("=" * 60)

    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('./result', exist_ok=True)

    try:
        # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        print("\n1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
        df_all, X_train_raw, X_test_raw, y_train, y_test, feature_cols = data_processing()

        # 2. íŠ¹ì„± ì„ íƒ (ê°„ë‹¨í•œ ë°©ë²•ë“¤)
        print("\n2ï¸âƒ£ íŠ¹ì„± ì„ íƒ")

        # ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ì œê±°
        remaining_features = remove_high_correlation_features(X_train_raw, threshold=0.9)
        X_train_filtered = X_train_raw[remaining_features]
        X_test_filtered = X_test_raw[remaining_features]

        # í†µê³„ì  ë°©ë²•ìœ¼ë¡œ ìµœê³  íŠ¹ì„± ì„ íƒ (ì˜µì…˜ 1)
        selected_features = select_best_features(X_train_filtered, y_train, k=50)

        # ë˜ëŠ” RFECV ì‚¬ìš© (ì˜µì…˜ 2 - ì£¼ì„ í•´ì œí•˜ë©´ ì‚¬ìš© ê°€ëŠ¥)
        # selected_features = simple_rfecv(X_train_filtered, y_train, max_features=50)

        print(f"ìµœì¢… ì„ íƒëœ íŠ¹ì„±: {len(selected_features)}ê°œ")

        # 3. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        print("\n3ï¸âƒ£ ë°ì´í„° ìŠ¤ì¼€ì¼ë§")
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train_filtered[selected_features])
        X_test = scaler.transform(X_test_filtered[selected_features])

        # DataFrame í˜•íƒœë¡œ ë³€í™˜ (ì»¬ëŸ¼ëª… ìœ ì§€)
        X_train = pd.DataFrame(X_train, columns=selected_features)
        X_test = pd.DataFrame(X_test, columns=selected_features)

        # 4. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ëª¨ë¸ ìƒì„±
        print("\n4ï¸âƒ£ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ëª¨ë¸ ìƒì„±")
        models = create_default_models()

        # 5. êµì°¨ ê²€ì¦
        print("\n5ï¸âƒ£ êµì°¨ ê²€ì¦ ìˆ˜í–‰")
        cv_results = cross_validate_models(models, X_train, y_train)

        # 6. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
        print("\n6ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€")
        results = []

        for model_name, model in models.items():
            metrics, y_pred, y_proba, trained_model = evaluate_model(
                model, model_name, X_train, X_test, y_train, y_test
            )

            if metrics is not None:
                # êµì°¨ ê²€ì¦ ê²°ê³¼ ì¶”ê°€
                metrics['CV_F1'] = cv_results[model_name]['CV_F1_mean']
                metrics['CV_F1_std'] = cv_results[model_name]['CV_F1_std']

                results.append({
                    'Model': model_name,
                    'Best Model': trained_model,
                    'y_pred': y_pred,
                    'y_proba': y_proba,
                    **metrics
                })

        # 7. ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
        print("\n7ï¸âƒ£ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”")

        if len(results) > 0:
            # ê²°ê³¼ DataFrame ìƒì„± ë° ì •ë ¬
            results_df = pd.DataFrame([{k: v for k, v in result.items()
                                        if k not in ['Best Model', 'y_pred', 'y_proba']}
                                       for result in results])
            results_df = results_df.sort_values('F1', ascending=False)

            # ì„±ëŠ¥ ìˆœìœ„ ì¶œë ¥
            print(f"\nğŸ† ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (F1 Score ê¸°ì¤€):")
            print("=" * 80)
            for i, row in results_df.iterrows():
                rank = len(results_df) - results_df.index.get_loc(i)
                print(f"{rank}. {row['Model']}")
                print(f"   í…ŒìŠ¤íŠ¸ F1: {row['F1']:.4f} | AUC: {row['AUC']:.4f} | Accuracy: {row['Accuracy']:.4f}")
                print(f"   êµì°¨ê²€ì¦ F1: {row['CV_F1']:.4f} Â± {row['CV_F1_std']:.4f}")
                print("-" * 60)

            # ì‹œê°í™”
            plot_model_comparison(results_df)
            plot_combined_roc_curves(results, X_test, y_test)

            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„
            best_result = results[results_df.index[0]]
            best_model = best_result['Best Model']
            best_model_name = best_result['Model']

            print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")

            # ë¶„ë¥˜ ë³´ê³ ì„œ
            print("\nğŸ“Š ë¶„ë¥˜ ë³´ê³ ì„œ:")
            print(classification_report(y_test, best_result['y_pred'],
                                        target_names=['Beginner', 'Trained']))

            # ì‹œê°í™”
            plot_confusion_matrix(y_test, best_result['y_pred'], best_model_name)

            if best_result['y_proba'] is not None:
                plot_roc_curve(y_test, best_result['y_proba'], best_model_name)

            plot_learning_curve(best_model, X_train, y_train, best_model_name)
            plot_feature_importance(best_model, X_train, y_train, selected_features, best_model_name)

            if best_model_name == 'Decision Tree':
                plot_decision_tree(best_model, selected_features)

            # ê²°ê³¼ ì €ì¥
            results_df.to_excel('./result/basic_ml_results.xlsx', index=False)
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: ./result/basic_ml_results.xlsx")

            # ì„ íƒëœ íŠ¹ì„± ì €ì¥
            feature_df = pd.DataFrame({'Selected_Features': selected_features})
            feature_df.to_excel('./result/selected_features.xlsx', index=False)
            print(f"ğŸ’¾ ì„ íƒëœ íŠ¹ì„± ì €ì¥: ./result/selected_features.xlsx")

        else:
            print("âŒ í‰ê°€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

    print(f"\nğŸ‰ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸