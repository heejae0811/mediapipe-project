 - 모수 (Parametric): 모델 형태를 미리 정해두고, 그 형태에 맞는 파라미터만 학습해서 예측
 Logis Regression, Linear Regression, Neural Network(모수지만 매우 유연)

 - 비모수 (Non-Parametric): 미리 정해진 함수 형태 없이, 데이터가 말해주는 대로 유연하게 모델을 만듦
 DT, RF, KNN, SVM(선형이면 모수, RBF면 비모수)

 ────────────────────────────────────────────

 - 과소적합 (Underfitting)
 모델이 너무 단순해서 훈련 데이터조차 잘 학습하지 못하는 상태
 Train Score: 낮음 / Test Score: 낮음

 - 과적합 (Overfitting)
 모델이 너무 복잡해서 훈련 데이터만 완벽히 맞추고, 새로운 데이터에는 성능이 떨어지는 상태
 Train Score: 매우 높음 / Test Score: 낮음

 "과소적합은 모델이 데이터를 충분히 설명하지 못하는 상태로, 훈련 및 검증 성능이 모두 낮다.
 반면 과적합은 모델이 훈련 데이터에만 과도하게 적합하여, 일반화 성능이 떨어지는 문제를 유발한다.
 따라서 적절한 하이퍼파라미터 조정 및 교차검증을 통해 이 두 극단을 피하는 것이 중요하다."

 ────────────────────────────────────────────

 1. Logistic Regression
 범주형 종속변수를 예측하는 확률 기반 선형(직선) 모델
 이진 분류 문제에서 주로 사용
 결과를 0-1 사이의 확률로 출력하고, 기준(threshold)을 넘으면 1로 예측
 빠르고 해석이 용이하지만, 비선형 관계에 약함
 ex) 이 사람이 숙련자일까?를 확률로 예측해서 선 하나로 나누는 모델

 2. K-Nearest Neighbors
 새로운 데이터를 예측할 때 가장 가까운 K개의 이웃(데이터)을 참조해서 다수결로 분류하는 모델
 학습 과정이 없음 (비모수적)
 거리 기반으로 분류 (보통 유클리드 거리 사용)
 직관적이고 구현이 쉽지만, 고차원에서 거리 계산이 왜곡됨 = 성능 저하
 ex) 이 사람 주변에 숙련자가 많으면 숙련자일 가능성이 높다고 판단

 3. Support Vector Machine
 클래스 간의 최적의 경계(Decision Boundary)를 찾는 지도 학습 모델
 마진을 최대로 하는 초평면을 찾음
 커널 트릭을 통해 비선형 분리도 가능
 적은 샘플에도 잘 작동되지만, 큰 데이터 셋에서는 느리고, 파라미터 조정이 필요
 ex) 두 그룹을 가장 깔끔하게 나눌 수 있는 선이나 곡선을 찾는 방법

 4. Decision Tree
 데이터를 조건에 따라 분기시켜 예측하는 트리 구조 모델
 전체 데이터에서 가장 잘 나눌 수 있는 기준을 선택해 더 이상 나눌 수 없거나 조건을 만족할 때까지 반복 = if-else 규칙 기반
 각 노드에서 가장 정보 이득이 큰 변수를 선택하여 분할
 시각화 용이하지만 과적합에 취약

 5. Random Forest
 여러 개의 결정 트리를 무작위로 학습시키고, 결과를 앙상블(다수결) 하는 모델
 부트스트래핑 + 랜덤 피처 선택
 평균화 또는 투표 방식으로 예측
 과적합을 줄이고 높은 성능이지만, 해석이 어렵고 학습 시간이 증가한다.

 6. LightGBM (Light Gradiend Boosting Machine)
 트리 기반 방식의 고속 고성능 모델
 리프 기반 트리 성장 방식
 대용량 데이터, 범주형 처리에 강함
 빠른 학습 속도와 높은 정확도를 가지지만, 소규모 데이터셋에서는 과적합 위험이 있다.

 7. XGBoost (Extreme Gradiend Boosting)
 Gradient Boosting을 정교하게 최적화한 트리 앙상블 기법
 정규화, 조기 종료, 결측값 자동 처리 등 내장
 반복적으로 약한 트리를 보완해가며 학습 = 실수 보완 방식
 성능, 안정성, 유연성이 우수하지만, 학습시간이 비교적 길다.

 8. CatBoost (Categorical Boosting)
 범주형 변수 처리에 최적화된 Gradient Boosting 모델
 자동으로 범주형 인코딩 수행
 순서 정보 보존, 과적합 방지 기법 내장
 범주형 데이터에 최적, 튜닝이 간편하지만, 초기 학습 시간이 다소 길다.

 ────────────────────────────────────────────

 1. Best Parameters (GridSearchCV)
 2. Best F1-score (CV 평균)
 3. Confusion Matrix
 4. ROC Curve
 5. Classification Report (생략 가능, Confusion Matrix에 포함돼도 됨)
 6. Feature Importance (Top 5~10)
 7. Train vs Test Accuracy (Bar chart)
 8. Learning Curve

 ────────────────────────────────────────────

 1. 문제 정의 및 목표 설정
 무엇을 예측/분류/설명하고 싶은가?
 이진 분류, 다중 분류, 회귀..

 2. 데이터 수집
 Label이 있어야 Supervised Machine Learning 가능

 3. 데이터 전처리
 결측지, 이상치 제거
 불필요한 열 제거

 4. X / y 정의
 X(Feature): 입력 변수 (거리, 시간, 속도..)
 y(Label, Target): 예측 대상 (숙련자, 비숙련자)

 5. Feature Selection
 모든 피처를 사용할 필요 없이
 Feature Importance, Permutation Importance, Correlation, SelectBest, PCA 또는 XAI로 중요 변수 추출

 6. Train / Test 데이터 분할
 train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

 7. 머신러닝 모델 선택 및 하이퍼파라미터 튜닝
 DT, RF, KNN, SVM..
 GridSearchCV, RandomizedSearchCV..
 scoring: f1, accuracy, roc_auc..
 random_state, cv, n_jobs, verbose..

 8. Classification 결과 예측
 model.predict(X_test)
 model.predict_proba(X_test): ROC, AUC

 9. 모델 평가
 Confusion Matrix
 Accuracy, Precision, Recall, F1-score
 ROC Curve, AUC
 Learning Curve

 10. XAI
 plot_tree()
 Feature Importance
 Permutation Importance
 SHAP
 LIME