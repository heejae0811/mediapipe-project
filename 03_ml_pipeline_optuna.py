import os
import glob
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, learning_curve, cross_val_score, GridSearchCV
from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import (
    confusion_matrix, classification_report, f1_score, matthews_corrcoef,
    precision_score, recall_score, roc_auc_score, roc_curve, auc,
    balanced_accuracy_score, ConfusionMatrixDisplay
)
from sklearn.inspection import permutation_importance

# Optuna import (ì„ íƒì )
try:
    import optuna
    from optuna.samplers import TPESampler
    from optuna.pruners import MedianPruner

    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install optuna'ë¡œ ì„¤ì¹˜í•˜ë©´ Optuna ìµœì í™”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings('ignore')

# Global Variable
RANDOM_STATE = 42


# ========================================
# Tuning Methods Enum
# ========================================
class TuningMethod:
    NONE = "none"  # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
    GRID_SEARCH = "grid"  # GridSearchCV
    OPTUNA = "optuna"  # Optuna ìµœì í™”


# ========================================
# Data Processing: ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬
# ========================================
def data_processing():
    """ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê¸°ë³¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    csv_files = glob.glob('./features_xlsx/*.xlsx')
    print(f"\nğŸ“‚ ë¶„ì„í•  íŒŒì¼ ìˆ˜ - {len(csv_files)}ê°œ")

    if len(csv_files) == 0:
        raise FileNotFoundError("ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    df_all = pd.concat([pd.read_excel(file, sheet_name=4) for file in csv_files], ignore_index=True)

    # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
    print(f"ì´ ë°ì´í„° ìˆ˜: {len(df_all)}ê°œ")
    print(f"ì»¬ëŸ¼ ìˆ˜: {df_all.shape[1]}ê°œ")

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ train/test split í›„ ì²˜ë¦¬ í•„ìš”
    print(f"ê²°ì¸¡ì¹˜ ê°œìˆ˜: {df_all.isnull().sum().sum()}ê°œ")

    # ë¼ë²¨ ì¸ì½”ë”©
    y_all = LabelEncoder().fit_transform(df_all['label'])
    print(f"ë¼ë²¨ ë¶„í¬: 0 - {(y_all == 0).sum()}ê°œ / 1 - {(y_all == 1).sum()}ê°œ")

    # ìˆ˜ì¹˜í˜• íŠ¹ì„±ë§Œ ì„ íƒ
    feature_cols = df_all.select_dtypes(include=['float64', 'int64']).columns.drop('label', errors='ignore')
    raw_features = df_all[feature_cols]

    print(f"ìµœì¢… íŠ¹ì„± ìˆ˜: {raw_features.shape[1]}ê°œ")

    # Train/Test ë¶„í• 
    X_train_raw, X_test_raw, y_train, y_test = train_test_split(
        raw_features, y_all, test_size=0.2, stratify=y_all, random_state=RANDOM_STATE
    )

    # ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€: í›ˆë ¨ ë°ì´í„°ë¡œë§Œ ê²°ì¸¡ì¹˜ì™€ ë¬´í•œê°’ ì²˜ë¦¬
    # í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬
    X_train_processed = X_train_raw.replace([np.inf, -np.inf], np.nan)
    train_median = X_train_processed.median()
    X_train_processed = X_train_processed.fillna(train_median)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” í›ˆë ¨ ë°ì´í„°ì˜ í†µê³„ëŸ‰ìœ¼ë¡œ ì²˜ë¦¬
    X_test_processed = X_test_raw.replace([np.inf, -np.inf], np.nan)
    X_test_processed = X_test_processed.fillna(train_median)

    return df_all, X_train_processed, X_test_processed, y_train, y_test, feature_cols


# ========================================
# Feature Selection: filter + embedded
# ========================================
def feature_selection(X, y, final_k=50):
    """íŠ¹ì„± ì„ íƒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    original_features = len(X.columns)

    # 1ë‹¨ê³„: ë¶„ì‚° í•„í„°ë§
    print("1. ë¶„ì‚° í•„í„°ë§ (Variance Threshold)")
    variances = X.var()
    low_var_threshold = 0.001
    low_variance_features = variances[variances <= low_var_threshold].index.tolist()
    remaining_features = [col for col in X.columns if col not in low_variance_features]
    X_filtered = X[remaining_features]

    print(f"   ì œê±°ëœ ë‚®ì€ ë¶„ì‚° íŠ¹ì„±: {len(low_variance_features)}ê°œ")
    print(f"   ë‚¨ì€ íŠ¹ì„±: {len(remaining_features)}ê°œ")

    # 2ë‹¨ê³„: ìƒê´€ê´€ê³„ í•„í„°ë§
    print("\n2. ìƒê´€ê´€ê³„ í•„í„°ë§ (Pearson Correlation)")
    corr_threshold = 0.90
    corr_matrix = X_filtered.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    highly_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > corr_threshold)]
    remaining_features = [col for col in remaining_features if col not in highly_corr_features]
    X_filtered = X_filtered[remaining_features]
    print(f"   ì œê±°ëœ ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„±: {len(highly_corr_features)}ê°œ")
    print(f"   ë‚¨ì€ íŠ¹ì„±: {len(remaining_features)}ê°œ")

    # 3ë‹¨ê³„: ANOVA F-testë¡œ 1ì°¨ ì„ ë³„
    intermediate_k = min(final_k * 2, len(remaining_features))
    if len(remaining_features) > intermediate_k:
        print(f"\n3. ANOVA F-testë¡œ 1ì°¨ ì„ ë³„ ({intermediate_k}ê°œ)")
        selector_anova = SelectKBest(score_func=f_classif, k=intermediate_k)
        selector_anova.fit(X_filtered, y)
        anova_features = X_filtered.columns[selector_anova.get_support()].tolist()
        X_filtered = X_filtered[anova_features]
        print(f"   ANOVA F-testë¡œ ì„ íƒëœ íŠ¹ì„±: {len(anova_features)}ê°œ")
        remaining_features = anova_features

    # 4ë‹¨ê³„: Mutual Informationìœ¼ë¡œ ìµœì¢… ì„ ë³„
    if len(remaining_features) > final_k:
        print(f"\n4. Mutual Informationìœ¼ë¡œ ìµœì¢… ì„ ë³„ ({final_k}ê°œ)")
        selector_mi = SelectKBest(
            score_func=lambda X, y: mutual_info_classif(X, y, random_state=RANDOM_STATE),
            k=final_k
        )
        selector_mi.fit(X_filtered, y)
        final_features = X_filtered.columns[selector_mi.get_support()].tolist()
        print(f"   Mutual Informationìœ¼ë¡œ ìµœì¢… ì„ íƒ: {len(final_features)}ê°œ")
    else:
        final_features = remaining_features
        print(f"\n4ï¸âƒ£ ì´ë¯¸ ëª©í‘œ íŠ¹ì„± ìˆ˜ ì´í•˜ì´ë¯€ë¡œ ëª¨ë“  íŠ¹ì„± ì‚¬ìš©: {len(final_features)}ê°œ")

    # ê²°ê³¼ ìš”ì•½
    print("=" * 50)
    print(f"\nâœ… íŠ¹ì„± ì„ íƒ ì™„ë£Œ!")
    print(f"   ì›ë³¸ íŠ¹ì„±: {original_features:4d}ê°œ")
    print(f"   ìµœì¢… ì„ íƒ: {len(final_features):4d}ê°œ")
    print(f"   ê°ì†Œìœ¨: {((original_features - len(final_features)) / original_features * 100):5.1f}%")
    print("=" * 50)

    return final_features


# ========================================
# ML Models Creation
# ========================================
def create_base_models():
    """ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return {
        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),
        'KNN': KNeighborsClassifier(n_jobs=-1),
        'Support Vector Machine': SVC(probability=True, random_state=RANDOM_STATE),
        'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),
        'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),
        'LightGBM': LGBMClassifier(random_state=RANDOM_STATE, verbosity=-1, n_jobs=-1),
        'XGBoost': XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss', verbosity=0, n_jobs=-1),
        'CatBoost': CatBoostClassifier(random_state=RANDOM_STATE, verbose=False),
    }


# ========================================
# GridSearch Hyperparameter Tuning
# ========================================
def get_grid_search_params():
    """GridSearchCVìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        'Logistic Regression': {
            'C': [0.01, 0.1, 1, 10, 100],
            'solver': ['liblinear', 'lbfgs'],
        },
        'KNN': {
            'n_neighbors': [3, 5, 7, 11, 15],
            'weights': ['uniform', 'distance']
        },
        'Support Vector Machine': {
            'C': [0.1, 1, 10],
            'kernel': ['rbf', 'linear'],
            'gamma': ['scale', 'auto']
        },
        'Decision Tree': {
            'max_depth': [3, 5, 10, None],
            'min_samples_split': [2, 5, 10],
            'criterion': ['gini', 'entropy']
        },
        'Random Forest': {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, None],
            'min_samples_split': [2, 5]
        },
        'LightGBM': {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.05, 0.1, 0.2],
            'max_depth': [3, 5, 7]
        },
        'XGBoost': {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.05, 0.1, 0.2],
            'max_depth': [3, 5, 7]
        },
        'CatBoost': {
            'iterations': [50, 100, 200],
            'learning_rate': [0.05, 0.1, 0.2],
            'depth': [3, 4, 5]
        }
    }


def grid_search_tuning(X_train, y_train):
    """GridSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤."""
    print("\nğŸ¯ GridSearch í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")

    base_models = create_base_models()
    param_grids = get_grid_search_params()
    tuned_models = {}
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)

    for model_name, base_model in base_models.items():
        print(f"âš¡ {model_name} íŠœë‹ ì¤‘...")

        try:
            if model_name in param_grids:
                search = GridSearchCV(
                    estimator=base_model,
                    param_grid=param_grids[model_name],
                    cv=cv,
                    scoring='f1',
                    n_jobs=-1,
                    verbose=0
                )
                search.fit(X_train, y_train)
                tuned_models[model_name] = search.best_estimator_
                print(f"   ìµœì  CV F1: {search.best_score_:.4f}")
            else:
                tuned_models[model_name] = base_model
                print(f"   ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
        except Exception as e:
            print(f"   âŒ íŠœë‹ ì‹¤íŒ¨: {e}")
            tuned_models[model_name] = base_model

    return tuned_models


# ========================================
# Optuna Hyperparameter Tuning
# ========================================
def create_optuna_objectives(X_train, y_train, cv_folds=3):
    """Optuna ëª©ì  í•¨ìˆ˜ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not OPTUNA_AVAILABLE:
        raise ImportError("Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)

    def run_cv(model):
        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1', n_jobs=-1)
        return scores.mean()

    def logistic_regression_objective(trial):
        params = {
            'C': trial.suggest_float('C', 0.001, 100, log=True),
            'solver': trial.suggest_categorical('solver', ['liblinear', 'lbfgs']),
            'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),
            'max_iter': 2000,
            'random_state': RANDOM_STATE
        }
        if params['penalty'] == 'l1' and params['solver'] == 'lbfgs':
            params['solver'] = 'liblinear'
        return run_cv(LogisticRegression(**params))

    def knn_objective(trial):
        params = {
            'n_neighbors': trial.suggest_int('n_neighbors', 3, 21, step=2),
            'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),
            'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan']),
            'n_jobs': -1
        }
        return run_cv(KNeighborsClassifier(**params))

    def svm_objective(trial):
        kernel = trial.suggest_categorical('kernel', ['linear', 'rbf'])
        params = {
            'C': trial.suggest_float('C', 0.01, 1000, log=True),
            'kernel': kernel,
            'probability': True,
            'random_state': RANDOM_STATE
        }
        if kernel == 'rbf':
            params['gamma'] = trial.suggest_categorical('gamma', ['scale', 'auto'])
        return run_cv(SVC(**params))

    def decision_tree_objective(trial):
        params = {
            'max_depth': trial.suggest_int('max_depth', 2, 8),
            'min_samples_split': trial.suggest_int('min_samples_split', 10, 50),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 25),
            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),
            'random_state': RANDOM_STATE
        }
        return run_cv(DecisionTreeClassifier(**params))

    def random_forest_objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 50, 300, step=50),
            'max_depth': trial.suggest_int('max_depth', 3, 15),
            'min_samples_split': trial.suggest_int('min_samples_split', 5, 30),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 15),
            'random_state': RANDOM_STATE,
            'n_jobs': -1
        }
        return run_cv(RandomForestClassifier(**params))

    def lightgbm_objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 50, 300, step=50),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'num_leaves': trial.suggest_int('num_leaves', 10, 100),
            'random_state': RANDOM_STATE,
            'verbosity': -1,
            'n_jobs': -1
        }
        return run_cv(LGBMClassifier(**params))

    def xgboost_objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 50, 300, step=50),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'max_depth': trial.suggest_int('max_depth', 3, 8),
            'random_state': RANDOM_STATE,
            'eval_metric': 'logloss',
            'verbosity': 0,
            'n_jobs': -1
        }
        return run_cv(XGBClassifier(**params))

    def catboost_objective(trial):
        params = {
            'iterations': trial.suggest_int('iterations', 50, 300, step=50),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'depth': trial.suggest_int('depth', 3, 8),
            'random_seed': RANDOM_STATE,
            'verbose': False
        }
        return run_cv(CatBoostClassifier(**params))

    return {
        'Logistic Regression': logistic_regression_objective,
        'KNN': knn_objective,
        'Support Vector Machine': svm_objective,
        'Decision Tree': decision_tree_objective,
        'Random Forest': random_forest_objective,
        'LightGBM': lightgbm_objective,
        'XGBoost': xgboost_objective,
        'CatBoost': catboost_objective
    }


def optuna_tuning(X_train, y_train, n_trials=50, timeout=300):
    """Optunaë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤."""
    if not OPTUNA_AVAILABLE:
        raise ImportError("Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install optuna'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

    print("\nğŸ¯ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")

    objectives = create_optuna_objectives(X_train, y_train)
    optimized_models = {}
    optuna_results = []

    sampler = TPESampler(seed=RANDOM_STATE)
    pruner = MedianPruner(n_startup_trials=5)

    for model_name, objective_func in objectives.items():
        print(f"âš¡ {model_name} ìµœì í™” ì¤‘...")

        study = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)
        study.optimize(objective_func, n_trials=n_trials, timeout=timeout)

        best_params = study.best_params
        print(f"   ìµœì  CV F1: {study.best_value:.4f}")

        optuna_results.append({
            "Model": model_name,
            "Best CV F1": study.best_value,
            "Best Params": best_params
        })

        # ìµœì  ëª¨ë¸ ìƒì„±
        if model_name == 'Logistic Regression':
            if best_params.get('penalty') == 'l1' and best_params.get('solver') == 'lbfgs':
                best_params['solver'] = 'liblinear'
            optimized_models[model_name] = LogisticRegression(**best_params)
        elif model_name == 'KNN':
            optimized_models[model_name] = KNeighborsClassifier(**best_params)
        elif model_name == 'Support Vector Machine':
            optimized_models[model_name] = SVC(**best_params)
        elif model_name == 'Decision Tree':
            optimized_models[model_name] = DecisionTreeClassifier(**best_params)
        elif model_name == 'Random Forest':
            optimized_models[model_name] = RandomForestClassifier(**best_params)
        elif model_name == 'LightGBM':
            optimized_models[model_name] = LGBMClassifier(**best_params)
        elif model_name == 'XGBoost':
            optimized_models[model_name] = XGBClassifier(**best_params)
        elif model_name == 'CatBoost':
            optimized_models[model_name] = CatBoostClassifier(**best_params)

    # ê²°ê³¼ ì €ì¥
    pd.DataFrame(optuna_results).to_excel('./result/optuna_results.xlsx', index=False)
    print("Optuna ìµœì í™” ê²°ê³¼ ì €ì¥: ./result/optuna_results.xlsx")

    return optimized_models


# ========================================
# Model Creation Controller
# ========================================
def create_ml_models(tuning_method=TuningMethod.NONE, X_train=None, y_train=None):
    """ì„ íƒëœ íŠœë‹ ë°©ë²•ì— ë”°ë¼ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""

    if tuning_method == TuningMethod.NONE:
        print("\nğŸ“¦ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")
        models = create_base_models()
        print(f"{len(models)}ê°œ ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
        return models

    elif tuning_method == TuningMethod.GRID_SEARCH:
        if X_train is None or y_train is None:
            raise ValueError("GridSearch íŠœë‹ì„ ìœ„í•œ í›ˆë ¨ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return grid_search_tuning(X_train, y_train)

    elif tuning_method == TuningMethod.OPTUNA:
        if X_train is None or y_train is None:
            raise ValueError("Optuna íŠœë‹ì„ ìœ„í•œ í›ˆë ¨ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return optuna_tuning(X_train, y_train)

    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŠœë‹ ë°©ë²•: {tuning_method}")


# ========================================
# Metrics & Evaluation Functions
# ========================================
def compute_metrics(y_true, y_pred, y_proba=None):
    """ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    cm = confusion_matrix(y_true, y_pred)
    if cm.size == 4:
        tn, fp, fn, tp = cm.ravel()
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    else:
        specificity = sensitivity = 0

    metrics = {
        'Accuracy': (y_true == y_pred).mean(),
        'Precision': precision_score(y_true, y_pred, zero_division=0),
        'Recall': recall_score(y_true, y_pred, zero_division=0),
        'F1': f1_score(y_true, y_pred, zero_division=0),
        'Balanced_Accuracy': balanced_accuracy_score(y_true, y_pred),
        'Specificity': specificity,
        'Sensitivity': sensitivity,
        'MCC': matthews_corrcoef(y_true, y_pred)
    }

    if y_proba is not None:
        try:
            metrics['AUC'] = roc_auc_score(y_true, y_proba)
        except:
            metrics['AUC'] = 0
    else:
        metrics['AUC'] = 0

    return metrics


def cross_validate_models(models, X, y, cv_folds=5):
    """êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print(f"\nğŸ”„ {cv_folds}-Fold êµì°¨ ê²€ì¦ ì‹¤í–‰ ì¤‘...")

    cv_results = {}
    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)

    for name, model in models.items():
        print(f"  âš¡ {name}")
        try:
            f1_scores = cross_val_score(model, X, y, cv=cv, scoring='f1', n_jobs=-1)
            cv_results[name] = {
                'CV_F1_mean': f1_scores.mean(),
                'CV_F1_std': f1_scores.std(),
                'CV_F1_scores': f1_scores
            }
            print(f"    CV F1: {f1_scores.mean():.4f} Â± {f1_scores.std():.4f}")
        except Exception as e:
            print(f"    âŒ {name} ê²€ì¦ ì‹¤íŒ¨: {e}")
            cv_results[name] = {
                'CV_F1_mean': 0,
                'CV_F1_std': 0,
                'CV_F1_scores': [0]
            }
    return cv_results


def evaluate_model(model, model_name, X_train, X_test, y_train, y_test):
    """ê°œë³„ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    print(f"\nğŸ“Š {model_name}")
    try:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        y_proba = None
        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_test)[:, 1]
        elif hasattr(model, 'decision_function'):
            y_proba = model.decision_function(X_test)

        metrics = compute_metrics(y_test, y_pred, y_proba)
        print(f"F1: {metrics['F1']:.4f}, AUC: {metrics['AUC']:.4f}, Accuracy: {metrics['Accuracy']:.4f}")

        return metrics, y_pred, y_proba, model
    except Exception as e:
        print(f"  âŒ {model_name} í‰ê°€ ì‹¤íŒ¨: {e}")
        return None, None, None, None


# ========================================
# Visualization Functions (ê°„ë‹¨í™”)
# ========================================
def plot_f1_comparison(results_df):
    """F1 ì ìˆ˜ ë¹„êµ ì°¨íŠ¸ë¥¼ ê·¸ë¦½ë‹ˆë‹¤."""
    results_df_sorted = results_df.sort_values('F1', ascending=False)
    plt.figure(figsize=(10, 6))
    colors = sns.color_palette("Set2", len(results_df_sorted))
    bars = plt.barh(results_df_sorted['Model'], results_df_sorted['F1'], color=colors)

    for i, bar in enumerate(bars):
        width = bar.get_width()
        plt.text(width + 0.01, bar.get_y() + bar.get_height() / 2, f'{width:.3f}', ha='left', va='center', fontsize=12)

    plt.title('F1 Score Comparison - All Models', fontsize=16, weight='bold')
    plt.xlabel('F1 Score')
    plt.grid(True, axis='x', linestyle='-', alpha=0.5)
    plt.tight_layout()
    plt.show()


def plot_roc_comparison(results, y_test):
    """ROC ê³¡ì„  ë¹„êµ ì°¨íŠ¸ë¥¼ ê·¸ë¦½ë‹ˆë‹¤."""
    plt.figure(figsize=(10, 6))
    colors = plt.cm.Set2.colors

    for i, result in enumerate(results):
        if result.get('Best Model') is None or result.get('y_proba') is None:
            continue

        name = result['Model']
        y_prob = result['y_proba']

        try:
            fpr, tpr, _ = roc_curve(y_test, y_prob)
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})',
                     color=colors[i % len(colors)], linewidth=2)
        except Exception as e:
            print(f"{name} ROC ê³¡ì„  ìƒì„± ì˜¤ë¥˜: {e}")
            continue

    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
    plt.title('ROC Curves - All Models', fontsize=16, weight='bold')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right', frameon=True)
    plt.grid(True, linestyle='-', alpha=0.5)
    plt.tight_layout()
    plt.show()


# ========================================
# Main Pipeline
# ========================================
def main(tuning_method=TuningMethod.NONE):
    """
    ë©”ì¸ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Parameters:
    - tuning_method: TuningMethod.NONE (ê¸°ë³¸ê°’), TuningMethod.GRID_SEARCH, TuningMethod.OPTUNA
    """
    print("ğŸš€ í†µí•© ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
    print(f"ğŸ”§ íŠœë‹ ë°©ë²•: {tuning_method}")

    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('./result', exist_ok=True)

    try:
        # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        print("\n1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
        df_all, X_train_raw, X_test_raw, y_train, y_test, feature_cols = data_processing()

        # 2. Feature Selection
        print("\n2ï¸âƒ£ Feature Selection")
        selected_features = feature_selection(X_train_raw, y_train, final_k=50)

        # 3. ëª¨ë¸ ìƒì„± (ì„ íƒëœ íŠœë‹ ë°©ë²•ì— ë”°ë¼)
        print("\n3ï¸âƒ£ ëª¨ë¸ ìƒì„±")
        if tuning_method == TuningMethod.NONE:
            models = create_ml_models(tuning_method)
        else:
            models = create_ml_models(tuning_method, X_train_raw[selected_features], y_train)

        # 4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        print("\n4ï¸âƒ£ ë°ì´í„° ìŠ¤ì¼€ì¼ë§")
        scale_needed = ['Logistic Regression', 'KNN', 'Support Vector Machine']

        scaled_X_train = {}
        scaled_X_test = {}

        for model_name in models.keys():
            scaler = StandardScaler()  # ê° ëª¨ë¸ë³„ë¡œ ìƒˆë¡œìš´ ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ ì‚¬ìš©

            if model_name in scale_needed:
                print(f"   ğŸ”¹ {model_name}: ìŠ¤ì¼€ì¼ë§ ì ìš©")
                X_train_scaled = scaler.fit_transform(X_train_raw[selected_features])
                X_test_scaled = scaler.transform(X_test_raw[selected_features])
                scaled_X_train[model_name] = pd.DataFrame(X_train_scaled, columns=selected_features)
                scaled_X_test[model_name] = pd.DataFrame(X_test_scaled, columns=selected_features)
            else:
                print(f"   âšª {model_name}: ìŠ¤ì¼€ì¼ë§ ë¯¸ì ìš©")
                scaled_X_train[model_name] = X_train_raw[selected_features].copy()
                scaled_X_test[model_name] = X_test_raw[selected_features].copy()

        # 5. êµì°¨ ê²€ì¦ ë° í‰ê°€
        print("\n5ï¸âƒ£ êµì°¨ ê²€ì¦ ë° í‰ê°€")
        cv_results = cross_validate_models(models, X_train_raw[selected_features], y_train)

        results = []
        for model_name, model in models.items():
            # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¡œ í‰ê°€
            X_train_eval = scaled_X_train[model_name]
            X_test_eval = scaled_X_test[model_name]

            metrics, y_pred, y_proba, trained_model = evaluate_model(
                model, model_name, X_train_eval, X_test_eval, y_train, y_test
            )

            if metrics is not None:
                # êµì°¨ ê²€ì¦ ê²°ê³¼ ì¶”ê°€
                metrics['CV_F1'] = cv_results[model_name]['CV_F1_mean']
                metrics['CV_F1_std'] = cv_results[model_name]['CV_F1_std']

                result_entry = {
                    'Model': model_name,
                    'Best Model': trained_model,
                    'y_pred': y_pred,
                    'y_proba': y_proba,
                    **metrics
                }
                results.append(result_entry)

        # 6. ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
        print("\n6ï¸âƒ£ ê²°ê³¼ ë¶„ì„")
        print("-" * 60)

        if len(results) > 0:
            # ê²°ê³¼ DataFrame ìƒì„± (ì‹œê°í™”ìš© ë°ì´í„°ë§Œ í¬í•¨)
            results_df = pd.DataFrame([
                {k: v for k, v in result.items() if k not in ['Best Model', 'y_pred', 'y_proba']}
                for result in results
            ])
            results_df = results_df.sort_values('F1', ascending=False)

            print(f"\nğŸ† ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (F1 Score ê¸°ì¤€):")
            for i, row in results_df.iterrows():
                print(
                    f"- {row['Model']} | F1={row['F1']:.4f}, AUC={row['AUC']:.4f}, Acc={row['Accuracy']:.4f} | CV_F1={row['CV_F1']:.4f} Â± {row['CV_F1_std']:.4f}")

            # ì‹œê°í™”
            plot_f1_comparison(results_df)
            plot_roc_comparison(results, y_test)

            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´
            best_result = results[0]  # F1 ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ ì²« ë²ˆì§¸ ê²°ê³¼
            best_model_name = best_result['Model']
            print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
            print("\nğŸ“Š ë¶„ë¥˜ ë³´ê³ ì„œ:")
            print(classification_report(y_test, best_result['y_pred'], target_names=['Beginner', 'Trained']))

            # ë…¼ë¬¸ìš© ì¢…í•© ê²°ê³¼ ì €ì¥
            save_comprehensive_results(results, results_df, selected_features, tuning_method, y_test)

            # í†µê³„ì  ìœ ì˜ì„± ê²€ì • (ë…¼ë¬¸ìš©)
            statistical_significance_test(results)

            print(f"\nğŸ’¾ ë…¼ë¬¸ìš© ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
            print(f"   - ì¢…í•© ì„±ëŠ¥ ê²°ê³¼: ./result/comprehensive_results.xlsx")
            print(f"   - ìƒì„¸ ë¶„ë¥˜ ê²°ê³¼: ./result/detailed_classification.xlsx")
            print(f"   - ì‹¤í—˜ ì„¤ì • ì •ë³´: ./result/experiment_config.xlsx")
            print(f"   - í†µê³„ì  ìœ ì˜ì„±: ./result/statistical_significance.xlsx")
            if tuning_method == TuningMethod.OPTUNA:
                print(f"   - Optuna ìµœì í™”: ./result/optuna_results.xlsx")

        else:
            print("âŒ í‰ê°€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

    print(f"\nğŸ‰ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")


# ========================================
# ë…¼ë¬¸ìš© ì¢…í•© ê²°ê³¼ ì €ì¥ í•¨ìˆ˜
# ========================================
def save_comprehensive_results(results, results_df, selected_features, tuning_method, y_test):
    """ë…¼ë¬¸ ë°œí‘œìš© ì¢…í•©ì ì¸ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""

    # ExcelWriter ê°ì²´ ìƒì„± (ë‹¤ì¤‘ ì‹œíŠ¸ ì €ì¥ìš©)
    with pd.ExcelWriter('./result/comprehensive_results.xlsx', engine='openpyxl') as writer:

        # Sheet 1: ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½
        performance_summary = results_df.copy()
        performance_summary = performance_summary.round(4)
        performance_summary.to_excel(writer, sheet_name='Performance_Summary', index=False)

        # Sheet 2: ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ (ëª¨ë“  ì§€í‘œ í¬í•¨)
        detailed_metrics = []
        for result in results:
            row = {
                'Model': result['Model'],
                'Accuracy': result['Accuracy'],
                'Precision': result['Precision'],
                'Recall': result['Recall'],
                'F1_Score': result['F1'],
                'AUC': result['AUC'],
                'Balanced_Accuracy': result['Balanced_Accuracy'],
                'Specificity': result['Specificity'],
                'Sensitivity': result['Sensitivity'],
                'MCC': result['MCC'],
                'CV_F1_Mean': result['CV_F1'],
                'CV_F1_Std': result['CV_F1_std']
            }
            detailed_metrics.append(row)

        detailed_df = pd.DataFrame(detailed_metrics).round(4)
        detailed_df.to_excel(writer, sheet_name='Detailed_Metrics', index=False)

        # Sheet 3: í˜¼ë™ í–‰ë ¬ (ëª¨ë“  ëª¨ë¸)
        confusion_matrices = []
        for result in results:
            if result['y_pred'] is not None:
                cm = confusion_matrix(y_test, result['y_pred'])
                if cm.size == 4:  # 2x2 matrix
                    tn, fp, fn, tp = cm.ravel()
                    confusion_matrices.append({
                        'Model': result['Model'],
                        'True_Negative': tn,
                        'False_Positive': fp,
                        'False_Negative': fn,
                        'True_Positive': tp,
                        'Total_Samples': len(y_test)
                    })

        confusion_df = pd.DataFrame(confusion_matrices)
        confusion_df.to_excel(writer, sheet_name='Confusion_Matrices', index=False)

        # Sheet 4: í´ë˜ìŠ¤ë³„ ë¶„ë¥˜ ë³´ê³ ì„œ
        classification_reports = []
        for result in results:
            if result['y_pred'] is not None:
                report = classification_report(
                    y_test, result['y_pred'],
                    target_names=['Beginner', 'Trained'],
                    output_dict=True
                )

                # Beginner í´ë˜ìŠ¤
                classification_reports.append({
                    'Model': result['Model'],
                    'Class': 'Beginner',
                    'Precision': report['Beginner']['precision'],
                    'Recall': report['Beginner']['recall'],
                    'F1_Score': report['Beginner']['f1-score'],
                    'Support': report['Beginner']['support']
                })

                # Trained í´ë˜ìŠ¤
                classification_reports.append({
                    'Model': result['Model'],
                    'Class': 'Trained',
                    'Precision': report['Trained']['precision'],
                    'Recall': report['Trained']['recall'],
                    'F1_Score': report['Trained']['f1-score'],
                    'Support': report['Trained']['support']
                })

                # Macro avg
                classification_reports.append({
                    'Model': result['Model'],
                    'Class': 'Macro_Avg',
                    'Precision': report['macro avg']['precision'],
                    'Recall': report['macro avg']['recall'],
                    'F1_Score': report['macro avg']['f1-score'],
                    'Support': report['macro avg']['support']
                })

                # Weighted avg
                classification_reports.append({
                    'Model': result['Model'],
                    'Class': 'Weighted_Avg',
                    'Precision': report['weighted avg']['precision'],
                    'Recall': report['weighted avg']['recall'],
                    'F1_Score': report['weighted avg']['f1-score'],
                    'Support': report['weighted avg']['support']
                })

        class_report_df = pd.DataFrame(classification_reports).round(4)
        class_report_df.to_excel(writer, sheet_name='Class_Reports', index=False)

        # Sheet 5: ì„ íƒëœ íŠ¹ì„± ì •ë³´
        feature_info = {
            'Feature_Name': selected_features,
            'Feature_Index': range(len(selected_features)),
            'Selection_Method': ['Multi-step Filter (Variance + Correlation + ANOVA + MI)'] * len(selected_features)
        }
        feature_df = pd.DataFrame(feature_info)
        feature_df.to_excel(writer, sheet_name='Selected_Features', index=False)

    # ë³„ë„ íŒŒì¼: ìƒì„¸ ë¶„ë¥˜ ê²°ê³¼
    with pd.ExcelWriter('./result/detailed_classification.xlsx', engine='openpyxl') as writer:

        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        for i, result in enumerate(results):
            if result['y_pred'] is not None:
                pred_results = pd.DataFrame({
                    'True_Label': y_test,
                    'Predicted_Label': result['y_pred'],
                    'Correct_Prediction': y_test == result['y_pred']
                })

                if result['y_proba'] is not None:
                    pred_results['Prediction_Probability'] = result['y_proba']

                sheet_name = result['Model'].replace(' ', '_')[:31]  # ì—‘ì…€ ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ
                pred_results.to_excel(writer, sheet_name=sheet_name, index=False)

    # ì‹¤í—˜ ì„¤ì • ì •ë³´ ì €ì¥
    experiment_config = {
        'Parameter': [
            'Random_State', 'Test_Size', 'CV_Folds', 'Feature_Selection_Method',
            'Final_Features_Count', 'Tuning_Method', 'Scaling_Applied_Models',
            'Total_Original_Features', 'Models_Evaluated'
        ],
        'Value': [
            RANDOM_STATE, 0.2, 5, 'Variance + Correlation + ANOVA + MI',
            len(selected_features), tuning_method, 'LR, KNN, SVM',
            'Dynamic', len(results)
        ],
        'Description': [
            'Fixed seed for reproducibility',
            'Proportion of data used for testing',
            'Number of cross-validation folds',
            'Multi-step feature selection approach',
            'Number of features after selection',
            'Hyperparameter optimization method',
            'Models that received feature scaling',
            'Number of features before selection',
            'Total number of models evaluated'
        ]
    }

    config_df = pd.DataFrame(experiment_config)
    config_df.to_excel('./result/experiment_config.xlsx', index=False)


# ========================================
# Statistical Significance Testing (ë…¼ë¬¸ìš© ì¶”ê°€ ê¸°ëŠ¥)
# ========================================
def statistical_significance_test(results):
    """ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ì˜ í†µê³„ì  ìœ ì˜ì„±ì„ ê²€ì •í•©ë‹ˆë‹¤."""
    from scipy import stats

    print("\nğŸ“ˆ í†µê³„ì  ìœ ì˜ì„± ê²€ì •")
    print("-" * 50)

    # CV F1 ì ìˆ˜ë“¤ì„ ì¶”ì¶œ
    cv_scores = {}
    for result in results:
        model_name = result['Model']
        # CV ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if 'CV_F1' in result and 'CV_F1_std' in result:
            # ì •ê·œë¶„í¬ ê°€ì •í•˜ì— CV ì ìˆ˜ë“¤ì„ ì‹œë®¬ë ˆì´ì…˜
            # (ì‹¤ì œë¡œëŠ” cross_val_scoreì˜ ê°œë³„ ì ìˆ˜ë“¤ì„ ì €ì¥í•´ì•¼ í•˜ì§€ë§Œ, í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¡œ ê·¼ì‚¬)
            mean_score = result['CV_F1']
            std_score = result['CV_F1_std']
            # 5-fold CV ê°€ì •
            simulated_scores = np.random.normal(mean_score, std_score, 5)
            cv_scores[model_name] = simulated_scores

    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ê³¼ ë‹¤ë¥¸ ëª¨ë¸ë“¤ ê°„ ë¹„êµ
    model_names = list(cv_scores.keys())
    if len(model_names) < 2:
        print("ë¹„êµí•  ëª¨ë¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return

    best_model = model_names[0]  # ì´ë¯¸ F1 ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆìŒ
    best_scores = cv_scores[best_model]

    significance_results = []

    for model_name in model_names[1:]:
        other_scores = cv_scores[model_name]

        # paired t-test ìˆ˜í–‰
        t_stat, p_value = stats.ttest_rel(best_scores, other_scores)

        significance_results.append({
            'Best_Model': best_model,
            'Compared_Model': model_name,
            'T_Statistic': t_stat,
            'P_Value': p_value,
            'Significant': p_value < 0.05,
            'Effect_Size': abs(t_stat) / np.sqrt(len(best_scores))  # Cohen's d ê·¼ì‚¬ì¹˜
        })

        significance_level = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"
        print(f"{best_model} vs {model_name}: t={t_stat:.3f}, p={p_value:.4f} {significance_level}")

    # í†µê³„ ê²°ê³¼ ì €ì¥
    if significance_results:
        sig_df = pd.DataFrame(significance_results).round(4)
        sig_df.to_excel('./result/statistical_significance.xlsx', index=False)
        print("\nğŸ’¾ í†µê³„ì  ìœ ì˜ì„± ê²€ì • ê²°ê³¼ ì €ì¥: ./result/statistical_significance.xlsx")


if __name__ == "__main__":
    # ì‚¬ìš©ë²• ì˜ˆì œ:

    # 1. ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ì‹¤í–‰
    # main(TuningMethod.NONE)

    # 2. GridSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í›„ ì‹¤í–‰
    # main(TuningMethod.GRID_SEARCH)

    # 3. Optunaë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í›„ ì‹¤í–‰ (Optuna ì„¤ì¹˜ í•„ìš”)
    # main(TuningMethod.OPTUNA)

    # ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹¤í–‰
    main(TuningMethod.NONE)