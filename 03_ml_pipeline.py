import os
import glob
import warnings
import random
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix, precision_score, recall_score, f1_score,
    matthews_corrcoef, roc_auc_score, balanced_accuracy_score,
    roc_curve, auc
)
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lime.lime_tabular import LimeTabularExplainer
import joblib


# =====================================================
# ì „ì—­ ì„¤ì •
# =====================================================
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
random.seed(RANDOM_STATE)

RESULT_DIR = "./result"
os.makedirs(RESULT_DIR, exist_ok=True)

warnings.filterwarnings("ignore")


# =====================================================
# 1ë‹¨ê³„: ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# =====================================================
def data_loading():
    print("\n[1ë‹¨ê³„] Data Loading")

    files = glob.glob("./features_xlsx/*.xlsx")
    print(f"ì°¾ì€ íŒŒì¼ ìˆ˜: {len(files)}")

    if len(files) == 0:
        raise FileNotFoundError("âŒ í´ë”ì— ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    df_list = [pd.read_excel(f) for f in files]
    df = pd.concat(df_list, ignore_index=True)

    if "label" not in df.columns:
        raise KeyError("âŒ label ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    y = df["label"].astype(int).values
    class_names = ["Intermediate", "Advanced"]
    label_counts = np.bincount(y)

    # ìˆ˜ì¹˜í˜• feature ìë™ ì„ íƒ
    feature_cols = df.select_dtypes(include=["float64", "int64"]).columns
    feature_cols = feature_cols.drop("label", errors="ignore")

    X = df[feature_cols]

    # ê²°ì¸¡ì¹˜ í™•ì¸
    total_missing = X.isnull().sum().sum()
    print(f"ê²°ì¸¡ì¹˜ ê°œìˆ˜: {total_missing}")

    if total_missing > 0:
        print("âš ï¸ ê²½ê³ : ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. \n")

    print(f"Feature ê°œìˆ˜: {len(feature_cols)}")
    print(f"ì‚¬ìš© í´ë˜ìŠ¤: {class_names}")
    print(f"í´ë˜ìŠ¤ ë¶„í¬: 0 - {label_counts[0]}ê°œ, 1 - {label_counts[1]}ê°œ")

    return X, y, list(feature_cols), class_names


# =====================================================
# 2ë‹¨ê³„: Data Split (Train/Test)
# =====================================================
def data_split(X, y, test_size=0.2):
    print("\n[2ë‹¨ê³„] Data Split (Train/Test)")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        stratify=y,
        random_state=RANDOM_STATE
    )

    print(f"Train ìƒ˜í”Œ ìˆ˜: {len(X_train)}")
    print(f"Test  ìƒ˜í”Œ ìˆ˜: {len(X_test)}")
    print(f"Train í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_train)}")
    print(f"Test  í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_test)}")

    return X_train, X_test, y_train, y_test


# =====================================================
# 3ë‹¨ê³„: Feature Selection
# =====================================================
def feature_selection_rfecv_rf(X_train, y_train, min_features=5):
    print("\n[3ë‹¨ê³„] Feature Selection (RFECV - RandomForest ê¸°ë°˜)")

    # Scaling (íŠ¸ë¦¬ì— ê¼­ í•„ìš”í•˜ì§„ ì•Šì§€ë§Œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„± ìœ ì§€)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    base_estimator = RandomForestClassifier(
        n_estimators=300,
        random_state=RANDOM_STATE,
        n_jobs=-1
    )

    cv = StratifiedKFold(
        n_splits=5, shuffle=True, random_state=RANDOM_STATE
    )

    rfecv = RFECV(
        estimator=base_estimator,
        step=1,
        cv=cv,
        scoring="matthews_corrcoef",
        min_features_to_select=min_features,
        n_jobs=-1
    )

    rfecv.fit(X_train_scaled, y_train)

    selected_features = X_train.columns[rfecv.support_]
    print(f"ì„ íƒëœ Feature ìˆ˜: {len(selected_features)}")
    print("Selected:", list(selected_features))

    return list(selected_features)


# =====================================================
# Feature Scaling
# =====================================================
def feature_scaling(X_train, X_test, selected_features, model_name):
    scaling_required = ["Logistic Regression", "KNN", "SVM"]

    if model_name not in scaling_required:
        print(f"âš  ìŠ¤ì¼€ì¼ë§ ìƒëµ: {model_name}")
        return X_train[selected_features], X_test[selected_features], None

    print(f"âœ” ìŠ¤ì¼€ì¼ë§ ì ìš©: {model_name}")

    scaler = StandardScaler()
    scaler.fit(X_train[selected_features])

    X_train_scaled = pd.DataFrame(
        scaler.transform(X_train[selected_features]),
        columns=selected_features,
        index=X_train.index
    )

    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test[selected_features]),
        columns=selected_features,
        index=X_test.index
    )

    return X_train_scaled, X_test_scaled, scaler


# =====================================================
# 4ë‹¨ê³„: ML ëª¨ë¸ ì •ì˜
# =====================================================
def model_development():
    print("\n[4ë‹¨ê³„] ML ëª¨ë¸ ìƒì„±")

    models = {
        # Linear models: ë°ì´í„°ê°€ ì„ í˜•ì ìœ¼ë¡œ ë¶„ë¦¬ë  ë•Œ íš¨ê³¼ì , í•´ì„ ê°€ëŠ¥ì„± ë†’ê³ , ì†ë„ ë¹ ë¦„
        "Logistic Regression": LogisticRegression(
            max_iter=1000,
            random_state=RANDOM_STATE
        ),
        # Non-Linear models (Distance-based / Kernel-based): ë³µì¡í•œ ê²°ì •ê²½ê³„ë¥¼ í•™ìŠµ ê°€ëŠ¥
        "KNN": KNeighborsClassifier(),
        "SVM": SVC(
            probability=True,  # í™•ë¥  ê¸°ë°˜ ì˜ˆì¸¡ â†’ XAI ìš©ë„
            random_state=RANDOM_STATE
        ),
        # Tree models
        "Decision Tree": DecisionTreeClassifier(
            random_state=RANDOM_STATE
        ),
        "Random Forest": RandomForestClassifier(
            n_estimators=200,
            random_state=RANDOM_STATE,
            n_jobs=-1
        ),
        # Gradient boosting models: ì‘ì€ ë°ì´í„°ì—ì„œë„ ê°•ë ¥í•œ ì„±ëŠ¥, ë³µì¡í•œ íŒ¨í„´ í•™ìŠµì— ì í•©
        "LightGBM": LGBMClassifier(
            random_state=RANDOM_STATE,
            n_jobs=-1,
            verbose=-1
        ),
        "XGBoost": XGBClassifier(
            random_state=RANDOM_STATE,
            eval_metric="logloss",
            n_jobs=-1,
            use_label_encoder=False
        ),
        "CatBoost": CatBoostClassifier(
            random_state=RANDOM_STATE,
            verbose=False
        )
    }

    print(f"ëª¨ë¸ ê°œìˆ˜: {len(models)}ê°œ")
    return models


# =====================================================
# 5ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# =====================================================
def model_evaluation(model, X_train, y_train, X_test, y_test, model_name):
    # 1) ëª¨ë¸ í•™ìŠµ
    model.fit(X_train, y_train)

    # 2) ì˜ˆì¸¡
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    # 3) ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()

    accuracy = (y_pred == y_test).mean()
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    bal_acc = balanced_accuracy_score(y_test, y_pred)
    mcc = matthews_corrcoef(y_test, y_pred)
    auc_score = roc_auc_score(y_test, y_proba)

    metrics = {
        "Model": model_name,
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1": f1,
        "Specificity": specificity,
        "Sensitivity": sensitivity,
        "Balanced_Accuracy": bal_acc,
        "MCC": mcc,
        "AUC": auc_score
    }

    print(f"- Accuracy: {accuracy:.3f}, F1: {f1:.3f}, MCC: {mcc:.3f}, AUC: {auc_score:.3f} \n")

    return model, y_pred, y_proba, mcc, metrics


# =====================================================
# 6ë‹¨ê³„: ì—‘ì…€ ì €ì¥ ë° ì‹œê°í™”
# =====================================================
def save_results(results_list, y_test, y_proba_dict, best_model_name, best_model, X_test_fs, selected_features, class_names):
    print("\n[6ë‹¨ê³„] ì—‘ì…€ ì €ì¥ ë° ì‹œê°í™”")

    df_results = pd.DataFrame(results_list)
    df_results_sorted = df_results.sort_values("MCC", ascending=False)

    # 1) ì—‘ì…€ ì €ì¥
    excel_path = os.path.join(RESULT_DIR, "final_results.xlsx")
    df_results_sorted.to_excel(excel_path, index=False)
    print(f"ì„±ëŠ¥ ì§€í‘œ ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {excel_path}")

    # 2) Confusion Matrix (Best Model)
    y_pred_best = best_model.predict(X_test_fs)
    cm = confusion_matrix(y_test, y_pred_best)

    plt.figure(figsize=(8, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.title(f"Confusion Matrix - {best_model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    cm_path = os.path.join(RESULT_DIR, "best_confusion_matrix.png")
    plt.savefig(cm_path, dpi=300)
    plt.close()
    print(f"Confusion Matrix ì €ì¥: {cm_path}")

    # 3) ROC Curve (ëª¨ë“  ëª¨ë¸ ë¹„êµ)
    plt.figure(figsize=(8, 5))
    for name, y_proba in y_proba_dict.items():
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f"{name} (AUC={roc_auc:.3f})")

    plt.plot([0, 1], [0, 1], "k--", label="Random")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve Comparison (All Models)")
    plt.legend(loc="lower right", fontsize=8)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    roc_all_path = os.path.join(RESULT_DIR, "all_roc_curve.png")
    plt.savefig(roc_all_path, dpi=300)
    plt.close()
    print(f"ROC Curve ì €ì¥: {roc_all_path}")

    # 4) Feature Importance (ì§€ì› ì•ˆë˜ë©´ Permutation Importance ì‚¬ìš©)
    if hasattr(best_model, "feature_importances_"):
        # Tree ëª¨ë¸ Feature Importance
        importances = best_model.feature_importances_
        df_fi = pd.DataFrame({
            "Feature": selected_features,
            "Importance": importances
        })
    else:
        # Permutation Importanceë¡œ ëŒ€ì²´
        from sklearn.inspection import permutation_importance

        perm = permutation_importance(
            best_model,
            X_test_fs,
            y_test,
            scoring="matthews_corrcoef",
            n_repeats=10,
            random_state=42
        )
        df_fi = pd.DataFrame({
            "Feature": selected_features,
            "Importance": perm.importances_mean
        })


# =====================================================
# 7ë‹¨ê³„: XAI (LIME)
# =====================================================
def xai_lime(best_model, X_train_fs, X_test_fs, selected_features, class_names):
    print("\n[7ë‹¨ê³„] XAI (LIME)")

    try:
        explainer = LimeTabularExplainer(
            training_data=np.array(X_train_fs),
            feature_names=selected_features,
            class_names=class_names,
            mode="classification"
        )

        # ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì„ íƒ
        sample = X_test_fs.iloc[0].values

        def predict_fn(x):
            return best_model.predict_proba(x)

        exp = explainer.explain_instance(sample, predict_fn)
        lime_path = os.path.join(RESULT_DIR, "best_lime_explanation.html")
        exp.save_to_file(lime_path)
        print(f"LIME ê²°ê³¼ ì €ì¥: {lime_path}")

    except Exception as e:
        print(f"âŒ LIME ì‹¤í–‰ ì‹¤íŒ¨: {e}")


# =====================================================
# 8ë‹¨ê³„: ì•Œê³ ë¦¬ì¦˜ ì €ì¥
# =====================================================
def save_algorithm(best_model, scaler, selected_features):
    print("\n[8ë‹¨ê³„] ì•Œê³ ë¦¬ì¦˜ ì €ì¥")

    model_path = "./result/best_model.pkl"
    scaler_path = "./result/best_scaler.pkl"
    features_path = "./result/best_features.pkl"

    joblib.dump(best_model, model_path)
    joblib.dump(scaler, scaler_path)
    joblib.dump(selected_features, features_path)

    print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
    print(f"ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: {scaler_path}")
    print(f"í”¼ì²˜ ì €ì¥ ì™„ë£Œ: {features_path}")


# =====================================================
# MAIN: ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸
# =====================================================
def main():
    print("\n============================================")
    print("ğŸš€ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("============================================")

    # 1ë‹¨ê³„: ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    X, y, feature_names, class_names = data_loading()

    # 2ë‹¨ê³„: Data Split (Train/Test)
    X_train, X_test, y_train, y_test = data_split(X, y)

    # 3ë‹¨ê³„: Feature Selection
    selected_features = feature_selection_rfecv_rf(
        X_train, y_train
    )
    print(selected_features)

    # 4ë‹¨ê³„: ML ëª¨ë¸ ì •ì˜
    models = model_development()

    best_model = None
    best_model_name = None
    best_mcc = -1
    best_scaler = None
    best_X_train_fs = None
    best_X_test_fs = None

    results_list = []
    y_proba_dict = {}

    # 5ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    print("\n[5ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")

    for model_name, model in models.items():
        X_train_fs, X_test_fs, scaler = feature_scaling(
            X_train, X_test, selected_features, model_name
        )

        model, y_pred, y_proba, mcc, metrics = model_evaluation(
            model, X_train_fs, y_train, X_test_fs, y_test, model_name
        )

        # ê²°ê³¼ ì €ì¥ìš©
        results_list.append(metrics)
        y_proba_dict[model_name] = y_proba

        # Best Model ê°±ì‹ 
        if mcc > best_mcc:
            best_mcc = mcc
            best_model_name = model_name
            best_model = model
            best_scaler = scaler
            best_X_train_fs = X_train_fs
            best_X_test_fs = X_test_fs

    print(f"âœ… Best Model (MCC ê¸°ì¤€): {best_model_name}")

    # 6ë‹¨ê³„: ì—‘ì…€ ì €ì¥ ë° ì‹œê°í™”
    save_results(
        results_list=results_list,
        y_test=y_test,
        y_proba_dict=y_proba_dict,
        best_model_name=best_model_name,
        best_model=best_model,
        X_test_fs=best_X_test_fs,
        selected_features=selected_features,
        class_names=class_names
    )

    # 7ë‹¨ê³„: XAI (LIME)
    xai_lime(best_model, best_X_train_fs, best_X_test_fs, selected_features, class_names)

    # 8ë‹¨ê³„: ì•Œê³ ë¦¬ì¦˜ ì €ì¥
    save_algorithm(best_model, best_scaler, selected_features)

    print("\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")


# =====================================================
# MAIN ì‹¤í–‰
# =====================================================
if __name__ == "__main__":
    main()
