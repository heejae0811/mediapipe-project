"""
ğŸ“š ë‹¨ê³„ë³„ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸
1ë‹¨ê³„: ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
2ë‹¨ê³„: Train/Test ë¶„ë¦¬ (8:2)
3ë‹¨ê³„: Data Scaling (í‘œì¤€í™”)
4ë‹¨ê³„: Feature Selection (RF ê¸°ë°˜ Top-K)
5ë‹¨ê³„: 8ê°œ ML ëª¨ë¸ í•™ìŠµ (ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°)
6ë‹¨ê³„: ML Evaluation (ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ê³„ì‚°)
7ë‹¨ê³„: ì—‘ì…€ ì €ì¥ ë° ì‹œê°í™”
8ë‹¨ê³„: XAI (LIME + SHAP)
"""

import os
import glob
import warnings
import random
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score, balanced_accuracy_score, roc_curve, auc)
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lime.lime_tabular import LimeTabularExplainer
import joblib


# =====================================================
# ì „ì—­ ì„¤ì •
# =====================================================
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
random.seed(RANDOM_STATE)

RESULT_DIR = "./result"
os.makedirs(RESULT_DIR, exist_ok=True)

warnings.filterwarnings("ignore")


# =====================================================
# 1ë‹¨ê³„: ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# =====================================================
def load_data():
    print("\n[1ë‹¨ê³„] ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°")

    files = glob.glob("./features_xlsx/*.xlsx")
    print(f"ì°¾ì€ íŒŒì¼ ìˆ˜: {len(files)}")

    if len(files) == 0:
        raise FileNotFoundError("âŒ features_xlsx í´ë”ì— ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ì—¬ëŸ¬ ê°œì˜ ì—‘ì…€ íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    df_list = [pd.read_excel(f, sheet_name=0) for f in files]
    df = pd.concat(df_list, ignore_index=True)

    # label ì¸ì½”ë”© (ë¬¸ì â†’ 0/1)
    le = LabelEncoder()
    y = le.fit_transform(df["label"])
    class_names = list(le.classes_)
    print(f"í´ë˜ìŠ¤ ë¶„í¬: {dict(zip(class_names, np.bincount(y)))}")

    # ìˆ«ìí˜• featureë§Œ ì‚¬ìš© (labelì€ ì œì™¸)
    feature_cols = df.select_dtypes(include=["float64", "int64"]).columns
    feature_cols = feature_cols.drop("label", errors="ignore")
    X = df[feature_cols]

    print(f"Feature ê°œìˆ˜: {len(feature_cols)}")
    print(f"ê²°ì¸¡ì¹˜ ê°œìˆ˜: {X.isnull().sum().sum()}")

    return X, y, list(feature_cols), class_names


# =====================================================
# 2ë‹¨ê³„: Train/Test ë¶„ë¦¬
# =====================================================
def split_data(X, y, test_size=0.2):
    print("\n[2ë‹¨ê³„] Train/Test ë¶„ë¦¬ (8:2)")

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=test_size,
        stratify=y,
        random_state=RANDOM_STATE
    )

    print(f"Train ìƒ˜í”Œ ìˆ˜: {len(X_train)}")
    print(f"Test ìƒ˜í”Œ ìˆ˜:  {len(X_test)}")
    print(f"Train í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_train)}")
    print(f"Test  í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_test)}")

    return X_train, X_test, y_train, y_test


# =====================================================
# 3ë‹¨ê³„: Data Scaling
# =====================================================
def scale_data(X_train, X_test):
    print("\n[3ë‹¨ê³„] Data Scaling")

    # ê²°ì¸¡ì¹˜ëŠ” ê° ì»¬ëŸ¼ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    X_train_filled = X_train.fillna(X_train.median())
    X_test_filled = X_test.fillna(X_train.median())  # Train ê¸°ì¤€ìœ¼ë¡œ ì±„ìš°ê¸°

    scaler = StandardScaler()
    scaler.fit(X_train_filled)

    X_train_scaled = pd.DataFrame(
        scaler.transform(X_train_filled),
        columns=X_train.columns,
        index=X_train.index
    )

    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test_filled),
        columns=X_test.columns,
        index=X_test.index
    )

    print("Scaling ì™„ë£Œ (í‰ê·  0, í‘œì¤€í¸ì°¨ 1 ê¸°ì¤€)")

    return X_train_scaled, X_test_scaled, scaler


# =====================================================
# 4ë‹¨ê³„: Feature Selection (RF ê¸°ë°˜ Top-K)
# =====================================================
def rf_importance_elbow(X_train, y_train, plot_path=None):
    """
    1) RFë¡œ feature importance ê³„ì‚°
    2) ì¤‘ìš”ë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    3) ì¤‘ìš”ë„ ì°¨ì´(derivative) ê³„ì‚°
    4) ê°€ì¥ í° ë³€í™”ëŸ‰(drop)ì´ ìˆëŠ” ì§€ì  â†’ elbow point = ìµœì  K
    """

    print("\n[4ë‹¨ê³„] Feature Selection")

    rf = RandomForestClassifier(n_estimators=600, random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train)

    importances = rf.feature_importances_
    idx_sorted = np.argsort(importances)[::-1]

    sorted_imp = importances[idx_sorted]
    sorted_feat = X_train.columns[idx_sorted]

    # ê¸°ìš¸ê¸°(ë³€í™”ëŸ‰) ê³„ì‚°
    diffs = np.diff(sorted_imp)

    # ê°€ì¥ í¬ê²Œ ë–¨ì–´ì§„ ì§€ì  = elbow
    elbow_k = np.argmin(diffs) + 1
    elbow_k = max(3, elbow_k)  # ìµœì†Œ 3ê°œ ì´ìƒ ë³´ì¥

    selected_features = list(sorted_feat[:elbow_k])

    # Plot ì €ì¥
    if plot_path:
        plt.figure(figsize=(7, 5))
        plt.plot(sorted_imp, marker="o")
        plt.axvline(elbow_k, color="red", linestyle="--", label=f"Elbow K={elbow_k}")
        plt.title("Random Forest Feature Importance Curve")
        plt.xlabel("Feature Rank")
        plt.ylabel("Importance")
        plt.legend()
        plt.tight_layout()
        plt.savefig(plot_path, dpi=300)
        plt.close()

    return selected_features, sorted_feat, sorted_imp, elbow_k


# =====================================================
# 5ë‹¨ê³„: 8ê°œ ML ëª¨ë¸ ì •ì˜ (ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°)
# =====================================================
def get_models():
    print("\n[5ë‹¨ê³„] ML ëª¨ë¸ ìƒì„±")

    models = {
        "Logistic Regression": LogisticRegression(
            max_iter=1000,
            random_state=RANDOM_STATE
        ),
        "KNN": KNeighborsClassifier(),
        "SVM": SVC(
            probability=True,
            random_state=RANDOM_STATE
        ),
        "Decision Tree": DecisionTreeClassifier(
            random_state=RANDOM_STATE
        ),
        "Random Forest": RandomForestClassifier(
            random_state=RANDOM_STATE,
            n_estimators=200,
            n_jobs=-1
        ),
        "LightGBM": LGBMClassifier(
            random_state=RANDOM_STATE,
            n_jobs=-1
        ),
        "XGBoost": XGBClassifier(
            random_state=RANDOM_STATE,
            eval_metric="logloss",
            n_jobs=-1,
            use_label_encoder=False
        ),
        "CatBoost": CatBoostClassifier(
            random_state=RANDOM_STATE,
            verbose=False
        ),
    }

    print(f"ëª¨ë¸ ê°œìˆ˜: {len(models)}ê°œ")
    return models


# =====================================================
# 6ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ + í‰ê°€
# =====================================================
def evaluate_models(models, X_train, y_train, X_test, y_test):
    print("\n[6ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")

    results_list = []
    y_proba_dict = {}
    model_objects = {}

    for name, model in models.items():
        print(f"\nâš¡ Training: {name}")

        # 1) ëª¨ë¸ í•™ìŠµ
        model.fit(X_train, y_train)

        # 2) ì˜ˆì¸¡ (ë¼ë²¨, í™•ë¥ )
        y_pred = model.predict(X_test)
        # ì´ì§„ë¶„ë¥˜ë¼ê³  ê°€ì •í•˜ê³ , ì–‘ì„± í´ë˜ìŠ¤(1)ì˜ í™•ë¥ ë§Œ ì‚¬ìš©
        y_proba = model.predict_proba(X_test)[:, 1]

        # 3) ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()

        accuracy = (y_pred == y_test).mean()
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)  # sensitivity
        f1 = f1_score(y_test, y_pred, zero_division=0)
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        bal_acc = balanced_accuracy_score(y_test, y_pred)
        mcc = matthews_corrcoef(y_test, y_pred)
        auc_score = roc_auc_score(y_test, y_proba)

        result = {
            "Model": name,
            "Accuracy": accuracy,
            "Precision": precision,
            "Recall": recall,
            "F1": f1,
            "Specificity": specificity,
            "Sensitivity": sensitivity,
            "Balanced_Accuracy": bal_acc,
            "MCC": mcc,
            "AUC": auc_score
        }
        results_list.append(result)

        y_proba_dict[name] = y_proba
        model_objects[name] = model

        print(f"   - Accuracy: {accuracy:.3f}, F1: {f1:.3f}, MCC: {mcc:.3f}, AUC: {auc_score:.3f}")

    # MCC ê¸°ì¤€ìœ¼ë¡œ Best Model ì„ ì •
    df_results = pd.DataFrame(results_list)
    best_idx = df_results["MCC"].idxmax()
    best_model_name = df_results.loc[best_idx, "Model"]
    best_model = model_objects[best_model_name]

    print(f"\nâœ… Best Model (MCC ê¸°ì¤€): {best_model_name}")

    return results_list, y_proba_dict, best_model_name, best_model


# =====================================================
# 7ë‹¨ê³„: ì—‘ì…€ ì €ì¥ ë° ì‹œê°í™”
# =====================================================
def save_results_and_plots(results_list, y_test, y_proba_dict, best_model_name, best_model, X_test_fs, selected_features, class_names):
    print("\n[7ë‹¨ê³„] ì—‘ì…€ ì €ì¥ ë° ì‹œê°í™”")

    df_results = pd.DataFrame(results_list)
    df_results_sorted = df_results.sort_values("MCC", ascending=False)

    # 1) ì—‘ì…€ ì €ì¥
    excel_path = os.path.join(RESULT_DIR, "final_results.xlsx")
    df_results_sorted.to_excel(excel_path, index=False)
    print(f"ì„±ëŠ¥ ì§€í‘œ ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {excel_path}")

    # 2) Confusion Matrix (Best Model)
    y_pred_best = best_model.predict(X_test_fs)
    cm = confusion_matrix(y_test, y_pred_best)

    plt.figure(figsize=(7, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.title(f"Confusion Matrix - {best_model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    cm_path = os.path.join(RESULT_DIR, "confusion_matrix_best.png")
    plt.savefig(cm_path, dpi=300)
    plt.close()
    print(f"Confusion Matrix ì €ì¥: {cm_path}")

    # 3) ROC Curve (ëª¨ë“  ëª¨ë¸ ë¹„êµ)
    plt.figure(figsize=(7, 6))
    for name, y_proba in y_proba_dict.items():
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f"{name} (AUC={roc_auc:.3f})")

    plt.plot([0, 1], [0, 1], "k--", label="Random")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve Comparison (All Models)")
    plt.legend(loc="lower right", fontsize=8)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    roc_all_path = os.path.join(RESULT_DIR, "roc_all_models.png")
    plt.savefig(roc_all_path, dpi=300)
    plt.close()
    print(f"ROC Curve ì €ì¥: {roc_all_path}")

    # 4) Feature Importance (ì§€ì› ì•ˆë˜ë©´ Permutation Importance ì‚¬ìš©)
    if hasattr(best_model, "feature_importances_"):
        # Tree ëª¨ë¸ Feature Importance
        importances = best_model.feature_importances_
        df_fi = pd.DataFrame({
            "Feature": selected_features,
            "Importance": importances
        })
    else:
        # Permutation Importanceë¡œ ëŒ€ì²´
        from sklearn.inspection import permutation_importance

        perm = permutation_importance(
            best_model,
            X_test_fs,
            y_test,
            scoring="matthews_corrcoef",
            n_repeats=10,
            random_state=42
        )
        df_fi = pd.DataFrame({
            "Feature": selected_features,
            "Importance": perm.importances_mean
        })


# =====================================================
# 8ë‹¨ê³„: XAI (LIME + SHAP)
# =====================================================
def run_xai(best_model, X_train_fs, X_test_fs, selected_features, class_names):
    """
    Best Modelì„ ëŒ€ìƒìœ¼ë¡œ LIME, SHAP ì‹¤í–‰.
    - LIME: ê°œë³„ ìƒ˜í”Œì— ëŒ€í•œ êµ­ì†Œ(local) ì„¤ëª…
    - SHAP: ì „ì²´ì ì¸(global) feature ì¤‘ìš”ë„ ì„¤ëª… (Tree ê¸°ë°˜ ëª¨ë¸ì—ì„œ)
    """
    print("\n[8ë‹¨ê³„] XAI (LIME + SHAP) ì‹¤í–‰")

    # ---------- LIME ----------
    print("LIME ì‹¤í–‰ ì¤‘...")
    try:
        explainer = LimeTabularExplainer(
            training_data=np.array(X_train_fs),
            feature_names=selected_features,
            class_names=class_names,
            mode="classification"
        )

        # ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì„ íƒ
        sample = X_test_fs.iloc[0].values

        def predict_fn(x):
            return best_model.predict_proba(x)

        exp = explainer.explain_instance(sample, predict_fn)
        lime_path = os.path.join(RESULT_DIR, "lime_explanation_best.html")
        exp.save_to_file(lime_path)
        print(f"LIME ê²°ê³¼ ì €ì¥: {lime_path}")
    except Exception as e:
        print(f"âŒ LIME ì‹¤í–‰ ì‹¤íŒ¨: {e}")


# =====================================================
# 9ë‹¨ê³„: ë² ìŠ¤íŠ¸ ëª¨ë¸ ì•Œê³ ë¦¬ì¦˜ ì €ì¥
# =====================================================
def save_final_artifacts(best_model, scaler, selected_features):
    print("\n[ì €ì¥ ë‹¨ê³„] ëª¨ë¸ / ìŠ¤ì¼€ì¼ëŸ¬ / í”¼ì²˜ ì €ì¥")

    model_path = "./result/best_model.pkl"
    scaler_path = "./result/scaler.pkl"
    features_path = "./result/selected_features.pkl"

    joblib.dump(best_model, model_path)
    joblib.dump(scaler, scaler_path)
    joblib.dump(selected_features, features_path)

    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
    print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: {scaler_path}")
    print(f"âœ… í”¼ì²˜ ì €ì¥ ì™„ë£Œ: {features_path}")


# =====================================================
# MAIN: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# =====================================================
def main():
    print("\n============================================")
    print("ğŸš€ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("============================================")

    # 1ë‹¨ê³„: ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    X, y, feature_names, class_names = load_data()

    # 2ë‹¨ê³„: Train/Test ë¶„ë¦¬
    X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2)

    # 3ë‹¨ê³„: Scaling
    X_train_scaled, X_test_scaled, scaler = scale_data(X_train, X_test)

    # 4ë‹¨ê³„: Feature Selection (RF Elbow ì ìš©)
    selected_features, sorted_feat, sorted_imp, K = rf_importance_elbow(
        X_train_scaled, y_train, "./result/rf_importance_curve.png"
    )
    print(f"Selected Feature({K}ê°œ): {selected_features}")

    X_train_fs = X_train_scaled[selected_features]
    X_test_fs = X_test_scaled[selected_features]

    # 5ë‹¨ê³„: ëª¨ë¸ ìƒì„±
    models = get_models()

    # 6ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ + í‰ê°€
    results_list, y_proba_dict, best_model_name, best_model = evaluate_models(
        models, X_train_fs, y_train, X_test_fs, y_test
    )

    # 7ë‹¨ê³„: ì‹œê°í™” ë° ì—‘ì…€ ì €ì¥
    save_results_and_plots(
        results_list, y_test, y_proba_dict,
        best_model_name, best_model,
        X_test_fs, selected_features, class_names
    )

    # 8ë‹¨ê³„: XAI (LIME)
    run_xai(best_model, X_train_fs, X_test_fs, selected_features, class_names)

    # 9ë‹¨ê³„: ì„œë²„ ì €ì¥
    save_final_artifacts(best_model, scaler, selected_features)

    print("\nğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ! result í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


# =====================================================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# =====================================================
if __name__ == "__main__":
    main()
