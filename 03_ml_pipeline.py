import os
import glob
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import (
    confusion_matrix, classification_report, f1_score, matthews_corrcoef,
    precision_score, recall_score, roc_auc_score, roc_curve, balanced_accuracy_score
)
from sklearn.pipeline import Pipeline
import optuna
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner


# ================================
# Global Variables
# ================================
warnings.filterwarnings('ignore')
RANDOM_STATE = 42
GLOBAL_CV = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)

class TuningMethod:
    DEFAULT = "default"
    GRID_SEARCH = "grid"
    OPTUNA = "optuna"


# ================================
# Data Processing
# ================================
def data_processing():
    files = glob.glob('./features_xlsx/*.xlsx')
    print(f"ğŸ“‚ ë¶„ì„í•  íŒŒì¼ ìˆ˜: {len(files)}ê°œ")

    if not files:
        raise FileNotFoundError("ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    df_all = pd.concat([pd.read_excel(file, sheet_name=4) for file in files], ignore_index=True)
    print(f"ì»¬ëŸ¼ ìˆ˜: {df_all.shape[1]}ê°œ")
    print(f"ê²°ì¸¡ì¹˜ ê°œìˆ˜: {df_all.isnull().sum().sum()}ê°œ")

    y_all = LabelEncoder().fit_transform(df_all['label'])
    print(f"ë¼ë²¨ ë¶„í¬: 0 - {(y_all == 0).sum()}ê°œ / 1 - {(y_all == 1).sum()}ê°œ")

    feature_cols = df_all.select_dtypes(include=['float64', 'int64']).columns.drop('label', errors='ignore')
    raw_features = df_all[feature_cols]
    X_train, X_test, y_train, y_test = train_test_split(
        raw_features, y_all, test_size=0.2, stratify=y_all, random_state=RANDOM_STATE
    )

    return df_all, X_train, X_test, y_train, y_test, feature_cols


# ================================
# Feature Selection: filter + embedded
# ================================
def feature_selection(X, y, final_k=50):
    print(f"\n{'=' * 60}")
    print(f"ğŸ‘‰ Feature Selection ì‹œì‘")
    print(f"{'=' * 60}")

    original_features = len(X.columns)

    # 1ë‹¨ê³„: ë¶„ì‚° í•„í„°ë§
    print("1. ë¶„ì‚° í•„í„°ë§ (Variance Threshold)")
    variances = X.var()
    low_var_threshold = 0.001
    low_variance_features = variances[variances <= low_var_threshold].index.tolist()
    remaining_features = [col for col in X.columns if col not in low_variance_features]
    X_filtered = X[remaining_features]

    print(f"   ì œê±°ëœ ë‚®ì€ ë¶„ì‚° íŠ¹ì„±: {len(low_variance_features)}ê°œ")
    print(f"   ë‚¨ì€ íŠ¹ì„±: {len(remaining_features)}ê°œ")

    # 2ë‹¨ê³„: ìƒê´€ê´€ê³„ í•„í„°ë§ (0.9 ì´ìƒ ì œê±°)
    print("\n2. ìƒê´€ê´€ê³„ í•„í„°ë§ (Pearson Correlation)")
    corr_threshold = 0.90
    corr_matrix = X_filtered.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    highly_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > corr_threshold)]
    remaining_features = [col for col in remaining_features if col not in highly_corr_features]
    X_filtered = X_filtered[remaining_features]
    print(f"   ì œê±°ëœ ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„±: {len(highly_corr_features)}ê°œ")
    print(f"   ë‚¨ì€ íŠ¹ì„±: {len(remaining_features)}ê°œ")

    # 3ë‹¨ê³„: ANOVA F-testë¡œ 1ì°¨ ì„ ë³„
    intermediate_k = min(final_k * 2, len(remaining_features))
    if len(remaining_features) > intermediate_k:
        print(f"\n3. ANOVA F-testë¡œ 1ì°¨ ì„ ë³„ ({intermediate_k}ê°œ)")

        selector_anova = SelectKBest(score_func=f_classif, k=intermediate_k)
        selector_anova.fit(X_filtered, y)

        anova_features = X_filtered.columns[selector_anova.get_support()].tolist()
        anova_scores = selector_anova.scores_[selector_anova.get_support()]
        X_filtered = X_filtered[anova_features]

        print(f"   ANOVA F-testë¡œ ì„ íƒëœ íŠ¹ì„±: {len(anova_features)}ê°œ")
        print(f"   í‰ê·  F-score: {anova_scores.mean():.2f}")
        remaining_features = anova_features

    # 4ë‹¨ê³„: Mutual Informationìœ¼ë¡œ ìµœì¢… ì„ ë³„
    if len(remaining_features) > final_k:
        print(f"\n4. Mutual Informationìœ¼ë¡œ ìµœì¢… ì„ ë³„ ({final_k}ê°œ)")

        selector_mi = SelectKBest(score_func=lambda X, y: mutual_info_classif(X, y, random_state=RANDOM_STATE), k=final_k)
        selector_mi.fit(X_filtered, y)
        final_features = X_filtered.columns[selector_mi.get_support()].tolist()
        mi_scores = selector_mi.scores_[selector_mi.get_support()]

        print(f"   Mutual Informationìœ¼ë¡œ ìµœì¢… ì„ íƒ: {len(final_features)}ê°œ")
        print(f"   í‰ê·  MI score: {mi_scores.mean():.3f}")
    else:
        final_features = remaining_features
        print(f"\n4. ì´ë¯¸ ëª©í‘œ íŠ¹ì„± ìˆ˜ ì´í•˜ì´ë¯€ë¡œ ëª¨ë“  íŠ¹ì„± ì‚¬ìš©: {len(final_features)}ê°œ")

    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š íŠ¹ì„± ì„ íƒ ìš”ì•½")
    print(f"   ì›ë³¸ íŠ¹ì„±: {original_features:4d}ê°œ")
    print(f"   ë¶„ì‚° í•„í„°ë§: {len(X.columns) - len(low_variance_features):4d}ê°œ (ì œê±°: {len(low_variance_features)}ê°œ)")
    print(f"   ìƒê´€ê´€ê³„ í•„í„°ë§: {len(remaining_features) + len(highly_corr_features):4d}ê°œ (ì œê±°: {len(highly_corr_features)}ê°œ)")
    if len(X_filtered.columns) != len(final_features):
        print(f"   ANOVA 1ì°¨: {len(X_filtered.columns):4d}ê°œ")
    print(f"   ìµœì¢… ì„ íƒ: {len(final_features):4d}ê°œ")
    print(f"   ê°ì†Œìœ¨: {((original_features - len(final_features)) / original_features * 100):5.1f}%")
    print("\nâœ… ì„ íƒ ë°©ë²•: ë¶„ì‚° â†’ ìƒê´€ê´€ê³„ â†’ ANOVA F-test â†’ Mutual Information")

    return final_features


# ================================
# Pipeline
# ================================
def make_pipeline_with_scaler(model, scaling=True):
    if scaling:
        scaler = StandardScaler()
    else:
        scaler = "passthrough"

    return Pipeline([("scaler", scaler), ("model", model)])


# ================================
# ML Models
# ================================
def get_all_models():
    scaling_models = {
        'Logistic Regression': LogisticRegression(
            random_state=RANDOM_STATE,
            max_iter=1000
        ),
        'K-Neighbors': KNeighborsClassifier(
            n_jobs=-1
        ),
        'Support Vector Machine': SVC(
            random_state=RANDOM_STATE,
            probability=True
        )
    }
    non_scaling_models = {
        'Decision Tree': DecisionTreeClassifier(
            random_state=RANDOM_STATE
        ),
        'Random Forest': RandomForestClassifier(
            random_state=RANDOM_STATE,
            n_jobs=-1
        ),
        'LightGBM': LGBMClassifier(
            random_state=RANDOM_STATE,
            n_jobs=-1,
            verbosity=-1
        ),
        'XGBoost': XGBClassifier(
            random_state=RANDOM_STATE,
            n_jobs=-1,
            verbosity=0
        ),
        'CatBoost': CatBoostClassifier(
            random_state=RANDOM_STATE,
            verbose=False
        )
    }

    models = {}
    for name, model in scaling_models.items():
        models[name] = make_pipeline_with_scaler(model, scaling=True)
    for name, model in non_scaling_models.items():
        models[name] = make_pipeline_with_scaler(model, scaling=False)

    return models


# ================================
# GridSearch Parameters
# ================================
def get_grid_search_params():
    return {
        'Logistic Regression': {
            'model__C': [0.01, 0.1, 1, 10, 100],
            'model__solver': ['liblinear', 'lbfgs'],
        },
        'K-Neighbors': {
            'model__n_neighbors': [3, 5, 7, 11, 15],
            'model__weights': ['uniform', 'distance']
        },
        'Support Vector Machine': {
            'model__C': [0.1, 1, 10],
            'model__kernel': ['rbf', 'linear'],
            'model__gamma': ['scale', 'auto']
        },
        'Decision Tree': {
            'model__max_depth': [3, 5, 10, None],
            'model__min_samples_split': [2, 5, 10],
            'model__criterion': ['gini', 'entropy']
        },
        'Random Forest': {
            'model__n_estimators': [50, 100, 200],
            'model__max_depth': [5, 10, None],
            'model__min_samples_split': [2, 5]
        },
        'LightGBM': {
            'model__n_estimators': [50, 100, 200],
            'model__learning_rate': [0.05, 0.1, 0.2],
            'model__max_depth': [3, 5, 7]
        },
        'XGBoost': {
            'model__n_estimators': [50, 100, 200],
            'model__learning_rate': [0.05, 0.1, 0.2],
            'model__max_depth': [3, 5, 7]
        },
        'CatBoost': {
            'model__iterations': [50, 100, 200],
            'model__learning_rate': [0.05, 0.1, 0.2],
            'model__depth': [3, 4, 5]
        }
    }


def grid_search_tuning(X_train, y_train):
    tuned = {}
    params = get_grid_search_params()
    models = get_all_models()

    for name, model in models.items():
        print(f"- {name} íŠœë‹")
        if name in params:
            search = GridSearchCV(model, params[name], cv=GLOBAL_CV, scoring='accuracy', n_jobs=-1, verbose=0)
            search.fit(X_train, y_train)
            tuned[name] = search.best_estimator_
            print(f"     Best params: {search.best_params_}")
            print(f"     Best CV Accuracy: {search.best_score_:.4f}")
        else:
            tuned[name] = model.fit(X_train, y_train)

    return tuned


# ================================
# Optuna
# ================================
def create_optuna_objective(model_name, X_train, y_train):
    def run_cv(pipeline):
        return cross_val_score(pipeline, X_train, y_train, cv=GLOBAL_CV, scoring='accuracy', n_jobs=-1).mean()

    def objective(trial):
        if model_name == "Logistic Regression":
            model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)
            params = {
                "model__C": trial.suggest_float("model__C", 0.001, 100, log=True),
                "model__solver": trial.suggest_categorical("model__solver", ["liblinear", "lbfgs"])
            }
            pipeline = make_pipeline_with_scaler(model, scaling=True).set_params(**params)

        elif model_name == "K-Neighbors":
            model = KNeighborsClassifier(n_jobs=-1)
            params = {
                "model__n_neighbors": trial.suggest_int("model__n_neighbors", 3, 15),
                "model__weights": trial.suggest_categorical("model__weights", ["uniform", "distance"])
            }
            pipeline = make_pipeline_with_scaler(model, scaling=True).set_params(**params)

        elif model_name == "Support Vector Machine":
            model = SVC(random_state=RANDOM_STATE, probability=True)
            params = {
                "model__C": trial.suggest_float("model__C", 0.01, 100, log=True),
                "model__kernel": trial.suggest_categorical("model__kernel", ["linear", "rbf"])
            }
            if params["model__kernel"] == "rbf":
                params["model__gamma"] = trial.suggest_categorical("model__gamma", ["scale", "auto"])
            pipeline = make_pipeline_with_scaler(model, scaling=True).set_params(**params)

        elif model_name == "Decision Tree":
            model = DecisionTreeClassifier(random_state=RANDOM_STATE)
            params = {
                "model__max_depth": trial.suggest_categorical("model__max_depth", [3, 5, 10, None]),
                "model__min_samples_split": trial.suggest_int("model__min_samples_split", 2, 10),
                "model__criterion": trial.suggest_categorical("model__criterion", ["gini", "entropy"])
            }
            pipeline = make_pipeline_with_scaler(model, scaling=False).set_params(**params)

        elif model_name == "Random Forest":
            model = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)
            params = {
                "model__n_estimators": trial.suggest_int("model__n_estimators", 50, 300),
                "model__max_depth": trial.suggest_categorical("model__max_depth", [5, 10, None]),
                "model__min_samples_split": trial.suggest_int("model__min_samples_split", 2, 5)
            }
            pipeline = make_pipeline_with_scaler(model, scaling=False).set_params(**params)

        elif model_name == "LightGBM":
            model = LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1)
            params = {
                "model__n_estimators": trial.suggest_int("model__n_estimators", 50, 300),
                "model__learning_rate": trial.suggest_float("model__learning_rate", 0.01, 0.3),
                "model__num_leaves": trial.suggest_int("model__num_leaves", 15, 63),
                "model__max_depth": trial.suggest_int("model__max_depth", 3, 7)
            }
            pipeline = make_pipeline_with_scaler(model, scaling=False).set_params(**params)

        elif model_name == "XGBoost":
            model = XGBClassifier(random_state=RANDOM_STATE, n_jobs=-1, verbosity=0)
            params = {
                "model__n_estimators": trial.suggest_int("model__n_estimators", 50, 300),
                "model__learning_rate": trial.suggest_float("model__learning_rate", 0.01, 0.3),
                "model__max_depth": trial.suggest_int("model__max_depth", 3, 7),
                "model__subsample": trial.suggest_float("model__subsample", 0.6, 1.0)
            }
            pipeline = make_pipeline_with_scaler(model, scaling=False).set_params(**params)

        elif model_name == "CatBoost":
            model = CatBoostClassifier(random_state=RANDOM_STATE, verbose=False)
            params = {
                "model__depth": trial.suggest_int("model__depth", 4, 10),
                "model__iterations": trial.suggest_int("model__iterations", 50, 300),
                "model__learning_rate": trial.suggest_float("model__learning_rate", 0.01, 0.3)
            }
            pipeline = make_pipeline_with_scaler(model, scaling=False).set_params(**params)

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")

        return run_cv(pipeline)

    return objective


def optuna_tuning(X_train, y_train, n_trials=30):
    optimized_models = {}
    optuna_results = []
    base_models = get_all_models()

    for name in base_models.keys():
        print(f"- {name} ìµœì í™”")

        objective = create_optuna_objective(name, X_train, y_train)
        study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=RANDOM_STATE), pruner=MedianPruner())
        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
        best_params = study.best_params
        best_model = base_models[name].set_params(**best_params)
        best_model.fit(X_train, y_train)
        optimized_models[name] = best_model
        optuna_results.append({'Model': name, 'Best Params': str(best_params), 'Best CV Accuracy': study.best_value})

        print(f"     Best params: {study.best_params}")
        print(f"     Best CV Accuracy: {study.best_value:.4f}")

    os.makedirs('./result', exist_ok=True)
    pd.DataFrame(optuna_results).to_excel('./result/results_optuna_details.xlsx', index=False)
    print("   Optuna ì„¸ë¶€ ê²°ê³¼ ì €ì¥: ./result/results_optuna_details.xlsx")

    return optimized_models


# ================================
# ML Evaluation
# ================================
def compute_metrics(y_true, y_pred, y_proba=None):
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)

    metrics = {
        'Accuracy': (y_true == y_pred).mean(),
        'Precision': precision_score(y_true, y_pred, zero_division=0),
        'Recall': recall_score(y_true, y_pred, zero_division=0),
        'F1': f1_score(y_true, y_pred, zero_division=0),
        'Balanced_Accuracy': balanced_accuracy_score(y_true, y_pred),
        'Specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,
        'Sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,
        'MCC': matthews_corrcoef(y_true, y_pred),
        'AUC': roc_auc_score(y_true, y_proba) if y_proba is not None else 0
    }
    return metrics


# ================================
# Visualization
# ================================
def plot_f1_comparison(results_df, tuning_method):
    subset = results_df[results_df['Tuning'] == tuning_method].sort_values('F1', ascending=False)

    plt.figure(figsize=(10, 6))
    bars = plt.barh(subset['Model'], subset['F1'], color=sns.color_palette("Set2", len(subset)))

    for bar, f1 in zip(bars, subset['F1']):
        plt.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2, f"{f1:.3f}", va="center", fontsize=10)

    plt.title(f"F1 Scores ({tuning_method.upper()})", fontsize=14, fontweight='bold')
    plt.xlabel("F1 Score", fontsize=12)
    plt.xlim(0, max(subset['F1']) * 1.1)
    plt.grid(True, axis='x', linestyle='-', alpha=0.5)
    plt.tight_layout()
    plt.show()


def plot_roc_comparison(results, y_test, tuning_method):
    plt.figure(figsize=(10, 6))
    subset = [r for r in results if r['Tuning'] == tuning_method]
    colors = sns.color_palette("Set2", len(subset))

    for i, r in enumerate(subset):
        if r['y_proba'] is not None and len(np.unique(r['y_proba'])) > 1:
            fpr, tpr, _ = roc_curve(y_test, r['y_proba'])
            plt.plot(fpr, tpr, label=f"{r['Model']} (AUC={r['AUC']:.3f})", color=colors[i], linewidth=2)

    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
    plt.title(f"ROC Curves ({tuning_method.upper()})", fontsize=14, fontweight='bold')
    plt.xlabel("False Positive Rate", fontsize=12)
    plt.ylabel("True Positive Rate", fontsize=12)
    plt.legend(loc="lower right")
    plt.grid(True, linestyle='-', alpha=0.5)
    plt.tight_layout()
    plt.show()


def plot_comprehensive_comparison(results_df):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    tuning_methods = results_df['Tuning'].unique()
    models = results_df['Model'].unique()
    x = np.arange(len(models))
    width = 0.25

    # F1 Score ë¹„êµ
    for i, tuning in enumerate(tuning_methods):
        subset = results_df[results_df['Tuning'] == tuning].set_index('Model')
        f1_scores = [subset.loc[model, 'F1'] if model in subset.index else 0 for model in models]
        ax1.bar(x + i * width, f1_scores, width, label=tuning.upper(), alpha=0.8)

    ax1.set_ylabel('F1 Score')
    ax1.set_title('F1 Score Comparison by Tuning Method')
    ax1.set_xticks(x + width)
    ax1.set_xticklabels(models, rotation=45, ha='right')
    ax1.legend()
    ax1.grid(True, linestyle='-', alpha=0.5)

    # AUC ë¹„êµ
    for i, tuning in enumerate(tuning_methods):
        subset = results_df[results_df['Tuning'] == tuning].set_index('Model')
        auc_scores = [subset.loc[model, 'AUC'] if model in subset.index else 0 for model in models]
        ax2.bar(x + i * width, auc_scores, width, label=tuning.upper(), alpha=0.8)

    ax2.set_ylabel('AUC Score')
    ax2.set_title('AUC Score Comparison by Tuning Method')
    ax2.set_xticks(x + width)
    ax2.set_xticklabels(models, rotation=45, ha='right')
    ax2.legend()
    ax2.grid(True, linestyle='-', alpha=0.5)

    plt.tight_layout()
    plt.show()


# ================================
# Save Results
# ================================
def save_results_by_tuning(results_df, y_test):
    os.makedirs('./result', exist_ok=True)

    # ì „ì²´ ê²°ê³¼ ì €ì¥
    results_df_save = results_df.drop(['y_pred', 'y_proba'], axis=1)
    results_df_save.to_excel('./result/final_results.xlsx', index=False)
    print("\nğŸ“Š ì „ì²´ ê²°ê³¼ ì €ì¥: ./result/final_results.xlsx")

    # íŠœë‹ë³„ ì €ì¥ ë° ì‹œê°í™”
    all_results = results_df.to_dict('records')

    for tuning in results_df['Tuning'].unique():
        subset = results_df[results_df['Tuning'] == tuning].drop(['y_pred', 'y_proba'], axis=1)
        subset.to_excel(f'./result/results_{tuning.upper()}.xlsx', index=False)
        print(f"   {tuning.upper()} ê²°ê³¼ ì €ì¥: ./result/results_{tuning.upper()}.xlsx")

        plot_f1_comparison(results_df, tuning)
        plot_roc_comparison(all_results, y_test, tuning)

    plot_comprehensive_comparison(results_df)

    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    best = results_df.sort_values('Accuracy', ascending=False).iloc[0]
    best_save = best.drop(['y_pred', 'y_proba'])
    pd.DataFrame([best_save]).to_excel('./result/best_model.xlsx', index=False)
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best['Model']} ({best['Tuning']}) | Accuracy={best['Accuracy']:.4f} | F1={best['F1']:.4f}")


# ================================
# Main
# ================================
def run_all():
    print("ğŸš€ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘ \n")

    # ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
    df_all, X_train, X_test, y_train, y_test, feature_cols = data_processing()

    # í´ë˜ìŠ¤ ê· í˜• í™•ì¸
    class_0_train = (y_train == 0).sum()
    class_1_train = (y_train == 1).sum()
    class_0_test = (y_test == 0).sum()
    class_1_test = (y_test == 1).sum()

    print(f"\nğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸")
    print(f"   Train: Class 0={class_0_train}ê°œ, Class 1={class_1_train}ê°œ")
    print(f"   Test:  Class 0={class_0_test}ê°œ, Class 1={class_1_test}ê°œ")
    print(f"   ê· í˜•ë„: {min(class_0_train, class_1_train) / max(class_0_train, class_1_train):.3f} (Train)")

    # íŠ¹ì„± ì„ íƒ (Train ë°ì´í„°ë§Œ ì‚¬ìš©)
    selected_features = feature_selection(X_train, y_train, final_k=50)
    X_train_selected = X_train[selected_features]
    X_test_selected = X_test[selected_features]

    all_results = []
    tuning_methods = [TuningMethod.DEFAULT, TuningMethod.GRID_SEARCH, TuningMethod.OPTUNA]

    for tuning_method in tuning_methods:
        print(f"\n{'=' * 60}")
        print(f"ğŸ¯ {tuning_method.upper()} ì‹¤í–‰")
        print(f"{'=' * 60}")

        # ëª¨ë¸ í•™ìŠµ ë° êµì°¨ê²€ì¦
        if tuning_method == TuningMethod.DEFAULT:
            models = get_all_models()
            tuned_models = {}
            for name, model in models.items():
                print(f"- {name} CV í‰ê°€")
                scores = cross_val_score(model, X_train_selected, y_train, cv=GLOBAL_CV, scoring='accuracy', n_jobs=-1)
                print(f"     CV Accuracy: {scores.mean():.4f} (Â±{scores.std():.4f})")
                model.fit(X_train_selected, y_train)
                tuned_models[name] = model
            models = tuned_models

        elif tuning_method == TuningMethod.GRID_SEARCH:
            models = grid_search_tuning(X_train_selected, y_train)

        elif tuning_method == TuningMethod.OPTUNA:
            models = optuna_tuning(X_train_selected, y_train, n_trials=30)

        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
        print(f"\nğŸ“Š {tuning_method.upper()} ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€:")
        for name, model in models.items():
            try:
                preds = model.predict(X_test_selected)

                if hasattr(model, "predict_proba"):
                    proba = model.predict_proba(X_test_selected)[:, 1]
                elif hasattr(model, "decision_function"):
                    proba = model.decision_function(X_test_selected)
                else:
                    proba = None

                metrics = compute_metrics(y_test, preds, proba)

                result = {
                    'Tuning': tuning_method,
                    'Model': name,
                    'y_pred': preds,
                    'y_proba': proba,
                    **metrics
                }
                all_results.append(result)

                print(
                    f"   {name:20s} | Accuracy: {metrics['Accuracy']:.4f} | F1: {metrics['F1']:.4f} | AUC: {metrics['AUC']:.4f}")

            except Exception as e:
                print(f"   âŒ {name} í‰ê°€ ì‹¤íŒ¨: {str(e)}")

    # ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥
    print(f"\n{'=' * 60}")
    print("ğŸ“ˆ ìµœì¢… ê²°ê³¼ ì •ë¦¬")
    print(f"{'=' * 60}")

    results_df = pd.DataFrame(all_results)

    print(f"\nğŸ… ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ìˆœìœ„ (Test Accuracy ê¸°ì¤€)")
    display_df = results_df[['Tuning', 'Model', 'Accuracy', 'F1', 'AUC']].sort_values('Accuracy', ascending=False)
    print(display_df.to_string(index=False, float_format='%.4f'))

    # ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
    save_results_by_tuning(results_df, y_test)

    # ìµœì¢… ì¶”ì²œ ëª¨ë¸
    best_model = results_df.sort_values('Accuracy', ascending=False).iloc[0]
    print(f"\nğŸ† ìµœì¢… ì¶”ì²œ ëª¨ë¸: {best_model['Model']} ({best_model['Tuning']})")
    print(f"   Test Accuracy: {best_model['Accuracy']:.4f} â­")
    print(f"   Test F1: {best_model['F1']:.4f} | Test AUC: {best_model['AUC']:.4f}")

    print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")


if __name__ == "__main__":
    run_all()